{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6e3e10-78d2-413c-87c3-f256294da062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor, log_evaluation, record_evaluation\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "#from sklearn.impute import IterativeImputer\n",
    "import pickle\n",
    "import optuna\n",
    "from optuna.visualization import plot_slice, plot_param_importances\n",
    "import shap\n",
    "import random\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#pl.Config.set_tbl_rows(-1)\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_fmt_str_lengths(10000)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a256f-c55b-415d-8d12-e753283ffee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'I:/Kaggle/jane-street-real-time-market-data-forecasting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d215c3-e2a4-4ea2-8125-708737b10ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.csv',\n",
       " 'imputed_train_ffill.parquet',\n",
       " 'kaggle_evaluation',\n",
       " 'lags.parquet',\n",
       " 'my_folder',\n",
       " 'responders.csv',\n",
       " 'sample_submission.csv',\n",
       " 'team_folder',\n",
       " 'test.parquet',\n",
       " 'top_100000_rows_sorted_by_weight_descending.parquet',\n",
       " 'top_10000_rows_sorted_by_weight_descending.parquet',\n",
       " 'train.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab47b6d-409b-41a7-a443-fcddc750f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47127338, 84)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (47_127_338, 84)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_31</th><th>feature_32</th><th>feature_33</th><th>feature_34</th><th>feature_35</th><th>feature_36</th><th>feature_37</th><th>feature_38</th><th>feature_39</th><th>feature_40</th><th>feature_41</th><th>feature_42</th><th>feature_43</th><th>feature_44</th><th>feature_45</th><th>feature_46</th><th>feature_47</th><th>feature_48</th><th>feature_49</th><th>feature_50</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_6</th></tr><tr><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i8</td><td>i8</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>3.889038</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.851033</td><td>0.242971</td><td>0.2634</td><td>-0.891687</td><td>11</td><td>7</td><td>76</td><td>-0.883028</td><td>0.003067</td><td>-0.744703</td><td>null</td><td>-0.169586</td><td>null</td><td>-1.335938</td><td>-1.707803</td><td>0.91013</td><td>null</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>null</td><td>null</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>null</td><td>null</td><td>null</td><td>0.28118</td><td>0.269163</td><td>0.349028</td><td>-0.012596</td><td>-0.225932</td><td>null</td><td>-1.073602</td><td>null</td><td>null</td><td>-0.181716</td><td>null</td><td>null</td><td>null</td><td>0.564021</td><td>2.088506</td><td>0.832022</td><td>null</td><td>0.204797</td><td>null</td><td>null</td><td>-0.808103</td><td>null</td><td>-2.037683</td><td>0.727661</td><td>null</td><td>-0.989118</td><td>-0.345213</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.251104</td><td>-0.110252</td><td>-0.491157</td><td>-1.02269</td><td>0.152241</td><td>-0.659864</td><td>null</td><td>null</td><td>-0.261412</td><td>-0.211486</td><td>-0.335556</td><td>-0.281498</td><td>0.775981</td></tr><tr><td>0</td><td>0</td><td>7</td><td>1.370613</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.676961</td><td>0.151984</td><td>0.192465</td><td>-0.521729</td><td>11</td><td>7</td><td>76</td><td>-0.865307</td><td>-0.225629</td><td>-0.582163</td><td>null</td><td>0.317467</td><td>null</td><td>-1.250016</td><td>-1.682929</td><td>1.412757</td><td>null</td><td>0.520378</td><td>0.744132</td><td>-0.788658</td><td>0.641776</td><td>null</td><td>null</td><td>0.2272</td><td>0.580907</td><td>1.128879</td><td>null</td><td>null</td><td>null</td><td>-1.512286</td><td>-1.414357</td><td>-1.823322</td><td>-0.082763</td><td>-0.184119</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-10.835207</td><td>-0.002704</td><td>-0.621836</td><td>null</td><td>1.172836</td><td>null</td><td>null</td><td>-1.625862</td><td>null</td><td>-1.410017</td><td>1.063013</td><td>null</td><td>0.888355</td><td>0.467994</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.065759</td><td>0.013322</td><td>-0.592855</td><td>-1.052685</td><td>-0.393726</td><td>-0.741603</td><td>null</td><td>null</td><td>-0.281207</td><td>-0.182894</td><td>-0.245565</td><td>-0.302441</td><td>0.703665</td></tr><tr><td>0</td><td>0</td><td>9</td><td>2.285698</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.056285</td><td>0.187227</td><td>0.249901</td><td>-0.77305</td><td>11</td><td>7</td><td>76</td><td>-0.675719</td><td>-0.199404</td><td>-0.586798</td><td>null</td><td>-0.814909</td><td>null</td><td>-1.296782</td><td>-2.040234</td><td>0.639589</td><td>null</td><td>1.597359</td><td>0.657514</td><td>-1.350148</td><td>0.364215</td><td>null</td><td>null</td><td>-0.017751</td><td>-0.317361</td><td>-0.122379</td><td>null</td><td>null</td><td>null</td><td>-0.320921</td><td>-0.95809</td><td>-2.436589</td><td>0.070999</td><td>-0.245239</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.420632</td><td>-3.515137</td><td>-4.67776</td><td>null</td><td>0.535897</td><td>null</td><td>null</td><td>-0.72542</td><td>null</td><td>-2.29417</td><td>1.764551</td><td>null</td><td>-0.120789</td><td>-0.063458</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.882604</td><td>-0.072482</td><td>-0.617934</td><td>-0.86323</td><td>-0.241892</td><td>-0.709919</td><td>null</td><td>null</td><td>0.377131</td><td>0.300724</td><td>-0.106842</td><td>-0.096792</td><td>2.109352</td></tr><tr><td>0</td><td>0</td><td>10</td><td>0.690606</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.139366</td><td>0.273328</td><td>0.306549</td><td>-1.262223</td><td>42</td><td>5</td><td>150</td><td>-0.694008</td><td>3.004091</td><td>0.114809</td><td>null</td><td>-0.251882</td><td>null</td><td>-1.902009</td><td>-0.979447</td><td>0.241165</td><td>null</td><td>-0.392359</td><td>-0.224699</td><td>-2.129397</td><td>-0.855287</td><td>null</td><td>null</td><td>0.404142</td><td>-0.578156</td><td>0.105702</td><td>null</td><td>null</td><td>null</td><td>0.544138</td><td>-0.087091</td><td>-1.500147</td><td>-0.201288</td><td>-0.038042</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.382074</td><td>2.669135</td><td>0.611711</td><td>null</td><td>2.413415</td><td>null</td><td>null</td><td>1.313203</td><td>null</td><td>-0.810125</td><td>2.939022</td><td>null</td><td>3.988801</td><td>1.834661</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.697595</td><td>1.074309</td><td>-0.206929</td><td>-0.530602</td><td>4.765215</td><td>0.571554</td><td>null</td><td>null</td><td>-0.226891</td><td>-0.251412</td><td>-0.215522</td><td>-0.296244</td><td>1.114137</td></tr><tr><td>0</td><td>0</td><td>14</td><td>0.44057</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.9552</td><td>0.262404</td><td>0.344457</td><td>-0.613813</td><td>44</td><td>3</td><td>16</td><td>-0.947351</td><td>-0.030018</td><td>-0.502379</td><td>null</td><td>0.646086</td><td>null</td><td>-1.844685</td><td>-1.58656</td><td>-0.182024</td><td>null</td><td>-0.969949</td><td>-0.673813</td><td>-1.282132</td><td>-1.399894</td><td>null</td><td>null</td><td>0.043815</td><td>-0.320225</td><td>-0.031713</td><td>null</td><td>null</td><td>null</td><td>-0.08842</td><td>-0.995003</td><td>-2.635336</td><td>-0.196461</td><td>-0.618719</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-2.0146</td><td>-2.321076</td><td>-3.711265</td><td>null</td><td>1.253902</td><td>null</td><td>null</td><td>0.476195</td><td>null</td><td>-0.771732</td><td>2.843421</td><td>null</td><td>1.379815</td><td>0.411827</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.948601</td><td>-0.136814</td><td>-0.447704</td><td>-1.141761</td><td>0.099631</td><td>-0.661928</td><td>null</td><td>null</td><td>3.678076</td><td>2.793581</td><td>2.61825</td><td>3.418133</td><td>-3.57282</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1698</td><td>967</td><td>34</td><td>3.242493</td><td>2.52516</td><td>-0.721981</td><td>2.544025</td><td>2.477615</td><td>0.417557</td><td>0.785812</td><td>1.117796</td><td>2.199436</td><td>0.415427</td><td>42</td><td>5</td><td>150</td><td>0.804403</td><td>1.157257</td><td>1.031543</td><td>-0.671189</td><td>-0.3286</td><td>-0.486132</td><td>1.730176</td><td>-0.006173</td><td>-0.001144</td><td>-0.213062</td><td>0.932618</td><td>1.367338</td><td>-0.238197</td><td>-0.692615</td><td>-0.121163</td><td>1.090798</td><td>1.444294</td><td>-0.675626</td><td>-1.013264</td><td>-0.242888</td><td>3.427639</td><td>-0.958278</td><td>3.139836</td><td>3.416278</td><td>-1.655316</td><td>-0.59944</td><td>-0.932876</td><td>2.493458</td><td>0.969462</td><td>1.102016</td><td>0.158982</td><td>-0.496177</td><td>0.036177</td><td>1.309866</td><td>0.828025</td><td>1.577955</td><td>1.040802</td><td>1.255398</td><td>2.577441</td><td>0.057455</td><td>0.953005</td><td>1.377051</td><td>-0.396358</td><td>0.520262</td><td>1.179617</td><td>1.127657</td><td>2.231928</td><td>0.614652</td><td>2.412886</td><td>-1.101531</td><td>-0.384833</td><td>-0.275818</td><td>-0.40804</td><td>2.427115</td><td>-0.108427</td><td>0.739734</td><td>0.830205</td><td>0.366287</td><td>1.33325</td><td>1.075499</td><td>1.798264</td><td>-0.183443</td><td>-0.190222</td><td>0.234211</td><td>0.347142</td><td>-0.044463</td><td>0.016936</td><td>-0.132337</td></tr><tr><td>1698</td><td>967</td><td>35</td><td>1.079139</td><td>1.857906</td><td>-0.790646</td><td>2.745439</td><td>2.339877</td><td>0.845065</td><td>0.65137</td><td>1.180301</td><td>1.966379</td><td>0.321543</td><td>25</td><td>7</td><td>195</td><td>-0.075294</td><td>-0.152726</td><td>-0.20417</td><td>-0.421137</td><td>0.21708</td><td>-0.258775</td><td>1.874978</td><td>0.19988</td><td>-0.199219</td><td>-0.125619</td><td>-1.004547</td><td>-0.051933</td><td>0.450905</td><td>0.009246</td><td>0.164127</td><td>-0.939974</td><td>-1.143421</td><td>-0.320071</td><td>-0.379835</td><td>-0.142429</td><td>3.862469</td><td>-1.451786</td><td>3.477489</td><td>2.861663</td><td>0.763459</td><td>0.075972</td><td>-0.119677</td><td>0.626035</td><td>0.148815</td><td>0.653281</td><td>0.059313</td><td>-0.845099</td><td>0.098528</td><td>0.409564</td><td>-0.675728</td><td>-0.011334</td><td>0.930534</td><td>0.83198</td><td>0.808955</td><td>0.219276</td><td>-0.315776</td><td>0.687755</td><td>-1.189577</td><td>0.180146</td><td>-0.175486</td><td>-1.60435</td><td>-0.209283</td><td>0.249847</td><td>0.288816</td><td>-1.101531</td><td>-0.343868</td><td>-0.253991</td><td>-0.278832</td><td>2.050639</td><td>-0.059506</td><td>-0.029396</td><td>-0.101381</td><td>-0.187759</td><td>-0.180839</td><td>-0.0861</td><td>-0.153405</td><td>-0.196077</td><td>-0.175292</td><td>1.04578</td><td>0.739733</td><td>0.03372</td><td>0.05086</td><td>-0.249584</td></tr><tr><td>1698</td><td>967</td><td>36</td><td>1.033172</td><td>2.515527</td><td>-0.672298</td><td>2.28925</td><td>2.521592</td><td>0.255077</td><td>0.919892</td><td>1.172018</td><td>2.180496</td><td>0.24846</td><td>49</td><td>7</td><td>297</td><td>1.026715</td><td>-0.096892</td><td>0.224309</td><td>-0.528109</td><td>-0.704952</td><td>-0.704818</td><td>2.312482</td><td>0.32804</td><td>-0.108193</td><td>null</td><td>-0.945684</td><td>-0.244173</td><td>0.205989</td><td>-0.357343</td><td>null</td><td>null</td><td>-1.11075</td><td>-0.580242</td><td>-0.400568</td><td>null</td><td>2.397877</td><td>-0.637258</td><td>3.260638</td><td>3.046786</td><td>0.440965</td><td>0.234842</td><td>-0.17558</td><td>1.022406</td><td>-0.500069</td><td>2.071033</td><td>0.413488</td><td>-0.450016</td><td>-0.156616</td><td>-0.253755</td><td>-0.769588</td><td>0.066086</td><td>0.047826</td><td>1.713707</td><td>0.772772</td><td>-0.549192</td><td>1.338474</td><td>0.933568</td><td>0.032978</td><td>-0.519118</td><td>-0.290343</td><td>-0.806786</td><td>0.106295</td><td>0.183461</td><td>1.830421</td><td>-1.101531</td><td>-0.341991</td><td>-0.249132</td><td>-0.34365</td><td>2.251358</td><td>0.601888</td><td>1.035051</td><td>-0.283241</td><td>0.107244</td><td>0.86016</td><td>0.024223</td><td>0.374852</td><td>-0.220933</td><td>-0.161584</td><td>0.032771</td><td>0.036888</td><td>0.168908</td><td>0.152333</td><td>-0.065355</td></tr><tr><td>1698</td><td>967</td><td>37</td><td>1.243116</td><td>2.663298</td><td>-0.889112</td><td>2.313155</td><td>3.101428</td><td>0.324454</td><td>0.618944</td><td>1.185663</td><td>1.599724</td><td>0.319719</td><td>34</td><td>4</td><td>214</td><td>0.759314</td><td>0.284057</td><td>0.41716</td><td>-0.611075</td><td>-0.513717</td><td>-0.891423</td><td>1.84994</td><td>0.406756</td><td>-1.608196</td><td>-0.252663</td><td>-0.271574</td><td>-0.051405</td><td>0.098146</td><td>-0.653961</td><td>0.173676</td><td>-0.016497</td><td>-0.404509</td><td>-0.577262</td><td>-0.731429</td><td>-0.21646</td><td>3.018564</td><td>-0.472061</td><td>3.13922</td><td>3.065858</td><td>0.842925</td><td>0.053283</td><td>-0.074403</td><td>0.500129</td><td>0.08263</td><td>0.336223</td><td>0.643934</td><td>-0.422367</td><td>-0.418195</td><td>0.203037</td><td>-0.702278</td><td>0.543305</td><td>-0.195764</td><td>0.693364</td><td>0.953293</td><td>0.352567</td><td>0.471775</td><td>1.876459</td><td>-0.143377</td><td>0.845516</td><td>0.301135</td><td>-0.395703</td><td>0.738038</td><td>-0.04124</td><td>1.270645</td><td>-1.101531</td><td>-0.358106</td><td>-0.141883</td><td>-0.255192</td><td>2.489247</td><td>0.537652</td><td>0.982107</td><td>-0.158009</td><td>0.137389</td><td>0.478357</td><td>0.782692</td><td>0.581421</td><td>-0.106056</td><td>-0.111017</td><td>0.163867</td><td>0.169331</td><td>-0.037563</td><td>-0.029483</td><td>-0.148711</td></tr><tr><td>1698</td><td>967</td><td>38</td><td>3.193685</td><td>2.728506</td><td>-0.745238</td><td>2.788789</td><td>2.343393</td><td>0.454731</td><td>0.862839</td><td>0.964795</td><td>2.089673</td><td>0.344931</td><td>50</td><td>1</td><td>522</td><td>0.406531</td><td>0.618247</td><td>1.01327</td><td>-0.952069</td><td>-0.679168</td><td>-0.597603</td><td>0.375125</td><td>1.97537</td><td>-0.440974</td><td>-0.072018</td><td>1.741353</td><td>1.380735</td><td>-0.110494</td><td>-0.874806</td><td>0.553424</td><td>0.532243</td><td>0.263214</td><td>-0.757856</td><td>-0.869204</td><td>-0.062955</td><td>3.619233</td><td>-0.386316</td><td>3.54456</td><td>3.120631</td><td>-1.443649</td><td>-0.257411</td><td>-0.309567</td><td>1.366358</td><td>-0.220885</td><td>0.029798</td><td>1.094489</td><td>-0.051078</td><td>-0.114243</td><td>0.517313</td><td>0.852201</td><td>0.522199</td><td>-0.027275</td><td>0.471593</td><td>1.213111</td><td>0.263278</td><td>0.915804</td><td>1.862022</td><td>0.503819</td><td>1.310126</td><td>0.662521</td><td>1.654948</td><td>1.090367</td><td>0.535922</td><td>0.653011</td><td>-1.101531</td><td>-0.622853</td><td>-0.363631</td><td>-0.395652</td><td>-0.016812</td><td>2.016734</td><td>0.241486</td><td>0.253229</td><td>0.228745</td><td>0.462717</td><td>0.799635</td><td>0.706102</td><td>-0.376377</td><td>-0.286764</td><td>-0.359046</td><td>-0.246135</td><td>-0.288941</td><td>-0.247774</td><td>-0.138548</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (47_127_338, 84)\n",
       "┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n",
       "│ dat ┆ tim ┆ sym ┆ wei ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ res │\n",
       "│ e_i ┆ e_i ┆ bol ┆ ght ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ pon │\n",
       "│ d   ┆ d   ┆ _id ┆ --- ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ der │\n",
       "│ --- ┆ --- ┆ --- ┆ f32 ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ _6  │\n",
       "│ i16 ┆ i16 ┆ i8  ┆     ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- │\n",
       "│     ┆     ┆     ┆     ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ i8  ┆ i8  ┆ i16 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 │\n",
       "╞═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╡\n",
       "│ 0   ┆ 0   ┆ 1   ┆ 3.8 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.8 ┆ 0.2 ┆ 0.2 ┆ -0. ┆ 11  ┆ 7   ┆ 76  ┆ -0. ┆ 0.0 ┆ -0. ┆ nul ┆ -0. ┆ nul ┆ -1. ┆ -1. ┆ 0.9 ┆ nul ┆ 1.6 ┆ 1.5 ┆ -1. ┆ -0. ┆ nul ┆ nul ┆ 1.3 ┆ -0. ┆ 0.1 ┆ nul ┆ nul ┆ nul ┆ 0.2 ┆ 0.2 ┆ 0.3 ┆ -0. ┆ -0. ┆ nul ┆ -1. ┆ nul ┆ nul ┆ -0. ┆ nul ┆ nul ┆ nul ┆ 0.5 ┆ 2.0 ┆ 0.8 ┆ nul ┆ 0.2 ┆ nul ┆ nul ┆ -0. ┆ nul ┆ -2. ┆ 0.7 ┆ nul ┆ -0. ┆ -0. ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -1. ┆ -0. ┆ -0. ┆ -1. ┆ 0.1 ┆ -0. ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 0.7 │\n",
       "│     ┆     ┆     ┆ 890 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 510 ┆ 429 ┆ 634 ┆ 891 ┆     ┆     ┆     ┆ 883 ┆ 030 ┆ 744 ┆ l   ┆ 169 ┆ l   ┆ 335 ┆ 707 ┆ 101 ┆ l   ┆ 364 ┆ 221 ┆ 551 ┆ 229 ┆ l   ┆ l   ┆ 783 ┆ 283 ┆ 231 ┆ l   ┆ l   ┆ l   ┆ 811 ┆ 691 ┆ 490 ┆ 012 ┆ 225 ┆ l   ┆ 073 ┆ l   ┆ l   ┆ 181 ┆ l   ┆ l   ┆ l   ┆ 640 ┆ 885 ┆ 320 ┆ l   ┆ 047 ┆ l   ┆ l   ┆ 808 ┆ l   ┆ 037 ┆ 276 ┆ l   ┆ 989 ┆ 345 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 251 ┆ 110 ┆ 491 ┆ 022 ┆ 522 ┆ 659 ┆ l   ┆ l   ┆ 261 ┆ 211 ┆ 335 ┆ 281 ┆ 759 │\n",
       "│     ┆     ┆     ┆ 38  ┆     ┆     ┆     ┆     ┆     ┆ 33  ┆ 71  ┆     ┆ 687 ┆     ┆     ┆     ┆ 028 ┆ 67  ┆ 703 ┆     ┆ 586 ┆     ┆ 938 ┆ 803 ┆ 3   ┆     ┆ 31  ┆ 33  ┆ 398 ┆ 627 ┆     ┆     ┆ 01  ┆ 712 ┆ 96  ┆     ┆     ┆     ┆ 8   ┆ 63  ┆ 28  ┆ 596 ┆ 932 ┆     ┆ 602 ┆     ┆     ┆ 716 ┆     ┆     ┆     ┆ 21  ┆ 06  ┆ 22  ┆     ┆ 97  ┆     ┆     ┆ 103 ┆     ┆ 683 ┆ 61  ┆     ┆ 118 ┆ 213 ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 104 ┆ 252 ┆ 157 ┆ 69  ┆ 41  ┆ 864 ┆     ┆     ┆ 412 ┆ 486 ┆ 556 ┆ 498 ┆ 81  │\n",
       "│ 0   ┆ 0   ┆ 7   ┆ 1.3 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.6 ┆ 0.1 ┆ 0.1 ┆ -0. ┆ 11  ┆ 7   ┆ 76  ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ 0.3 ┆ nul ┆ -1. ┆ -1. ┆ 1.4 ┆ nul ┆ 0.5 ┆ 0.7 ┆ -0. ┆ 0.6 ┆ nul ┆ nul ┆ 0.2 ┆ 0.5 ┆ 1.1 ┆ nul ┆ nul ┆ nul ┆ -1. ┆ -1. ┆ -1. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -10 ┆ -0. ┆ -0. ┆ nul ┆ 1.1 ┆ nul ┆ nul ┆ -1. ┆ nul ┆ -1. ┆ 1.0 ┆ nul ┆ 0.8 ┆ 0.4 ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -1. ┆ 0.0 ┆ -0. ┆ -1. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 0.7 │\n",
       "│     ┆     ┆     ┆ 706 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 769 ┆ 519 ┆ 924 ┆ 521 ┆     ┆     ┆     ┆ 865 ┆ 225 ┆ 582 ┆ l   ┆ 174 ┆ l   ┆ 250 ┆ 682 ┆ 127 ┆ l   ┆ 203 ┆ 441 ┆ 788 ┆ 417 ┆ l   ┆ l   ┆ 272 ┆ 809 ┆ 288 ┆ l   ┆ l   ┆ l   ┆ 512 ┆ 414 ┆ 823 ┆ 082 ┆ 184 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ .83 ┆ 002 ┆ 621 ┆ l   ┆ 728 ┆ l   ┆ l   ┆ 625 ┆ l   ┆ 410 ┆ 630 ┆ l   ┆ 883 ┆ 679 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 065 ┆ 133 ┆ 592 ┆ 052 ┆ 393 ┆ 741 ┆ l   ┆ l   ┆ 281 ┆ 182 ┆ 245 ┆ 302 ┆ 036 │\n",
       "│     ┆     ┆     ┆ 13  ┆     ┆     ┆     ┆     ┆     ┆ 61  ┆ 84  ┆ 65  ┆ 729 ┆     ┆     ┆     ┆ 307 ┆ 629 ┆ 163 ┆     ┆ 67  ┆     ┆ 016 ┆ 929 ┆ 57  ┆     ┆ 78  ┆ 32  ┆ 658 ┆ 76  ┆     ┆     ┆     ┆ 07  ┆ 79  ┆     ┆     ┆     ┆ 286 ┆ 357 ┆ 322 ┆ 763 ┆ 119 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 520 ┆ 704 ┆ 836 ┆     ┆ 36  ┆     ┆     ┆ 862 ┆     ┆ 017 ┆ 13  ┆     ┆ 55  ┆ 94  ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 759 ┆ 22  ┆ 855 ┆ 685 ┆ 726 ┆ 603 ┆     ┆     ┆ 207 ┆ 894 ┆ 565 ┆ 441 ┆ 65  │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ 0   ┆ 0   ┆ 9   ┆ 2.2 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 1.0 ┆ 0.1 ┆ 0.2 ┆ -0. ┆ 11  ┆ 7   ┆ 76  ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ -0. ┆ nul ┆ -1. ┆ -2. ┆ 0.6 ┆ nul ┆ 1.5 ┆ 0.6 ┆ -1. ┆ 0.3 ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -2. ┆ 0.0 ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -1. ┆ -3. ┆ -4. ┆ nul ┆ 0.5 ┆ nul ┆ nul ┆ -0. ┆ nul ┆ -2. ┆ 1.7 ┆ nul ┆ -0. ┆ -0. ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ 0.3 ┆ 0.3 ┆ -0. ┆ -0. ┆ 2.1 │\n",
       "│     ┆     ┆     ┆ 856 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 562 ┆ 872 ┆ 499 ┆ 773 ┆     ┆     ┆     ┆ 675 ┆ 199 ┆ 586 ┆ l   ┆ 814 ┆ l   ┆ 296 ┆ 040 ┆ 395 ┆ l   ┆ 973 ┆ 575 ┆ 350 ┆ 642 ┆ l   ┆ l   ┆ 017 ┆ 317 ┆ 122 ┆ l   ┆ l   ┆ l   ┆ 320 ┆ 958 ┆ 436 ┆ 709 ┆ 245 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 420 ┆ 515 ┆ 677 ┆ l   ┆ 358 ┆ l   ┆ l   ┆ 725 ┆ l   ┆ 294 ┆ 645 ┆ l   ┆ 120 ┆ 063 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 882 ┆ 072 ┆ 617 ┆ 863 ┆ 241 ┆ 709 ┆ l   ┆ l   ┆ 771 ┆ 007 ┆ 106 ┆ 096 ┆ 093 │\n",
       "│     ┆     ┆     ┆ 98  ┆     ┆     ┆     ┆     ┆     ┆ 85  ┆ 27  ┆ 01  ┆ 05  ┆     ┆     ┆     ┆ 719 ┆ 404 ┆ 798 ┆     ┆ 909 ┆     ┆ 782 ┆ 234 ┆ 89  ┆     ┆ 59  ┆ 14  ┆ 148 ┆ 15  ┆     ┆     ┆ 751 ┆ 361 ┆ 379 ┆     ┆     ┆     ┆ 921 ┆ 09  ┆ 589 ┆ 99  ┆ 239 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 632 ┆ 137 ┆ 76  ┆     ┆ 97  ┆     ┆     ┆ 42  ┆     ┆ 17  ┆ 51  ┆     ┆ 789 ┆ 458 ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 604 ┆ 482 ┆ 934 ┆ 23  ┆ 892 ┆ 919 ┆     ┆     ┆ 31  ┆ 24  ┆ 842 ┆ 792 ┆ 52  │\n",
       "│ 0   ┆ 0   ┆ 10  ┆ 0.6 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 1.1 ┆ 0.2 ┆ 0.3 ┆ -1. ┆ 42  ┆ 5   ┆ 150 ┆ -0. ┆ 3.0 ┆ 0.1 ┆ nul ┆ -0. ┆ nul ┆ -1. ┆ -0. ┆ 0.2 ┆ nul ┆ -0. ┆ -0. ┆ -2. ┆ -0. ┆ nul ┆ nul ┆ 0.4 ┆ -0. ┆ 0.1 ┆ nul ┆ nul ┆ nul ┆ 0.5 ┆ -0. ┆ -1. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.3 ┆ 2.6 ┆ 0.6 ┆ nul ┆ 2.4 ┆ nul ┆ nul ┆ 1.3 ┆ nul ┆ -0. ┆ 2.9 ┆ nul ┆ 3.9 ┆ 1.8 ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -0. ┆ 1.0 ┆ -0. ┆ -0. ┆ 4.7 ┆ 0.5 ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 1.1 │\n",
       "│     ┆     ┆     ┆ 906 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 393 ┆ 733 ┆ 065 ┆ 262 ┆     ┆     ┆     ┆ 694 ┆ 040 ┆ 148 ┆ l   ┆ 251 ┆ l   ┆ 902 ┆ 979 ┆ 411 ┆ l   ┆ 392 ┆ 224 ┆ 129 ┆ 855 ┆ l   ┆ l   ┆ 041 ┆ 578 ┆ 057 ┆ l   ┆ l   ┆ l   ┆ 441 ┆ 087 ┆ 500 ┆ 201 ┆ 038 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 820 ┆ 691 ┆ 117 ┆ l   ┆ 134 ┆ l   ┆ l   ┆ 132 ┆ l   ┆ 810 ┆ 390 ┆ l   ┆ 888 ┆ 346 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 697 ┆ 743 ┆ 206 ┆ 530 ┆ 652 ┆ 715 ┆ l   ┆ l   ┆ 226 ┆ 251 ┆ 215 ┆ 296 ┆ 141 │\n",
       "│     ┆     ┆     ┆ 06  ┆     ┆     ┆     ┆     ┆     ┆ 66  ┆ 28  ┆ 49  ┆ 223 ┆     ┆     ┆     ┆ 008 ┆ 91  ┆ 09  ┆     ┆ 882 ┆     ┆ 009 ┆ 447 ┆ 65  ┆     ┆ 359 ┆ 699 ┆ 397 ┆ 287 ┆     ┆     ┆ 42  ┆ 156 ┆ 02  ┆     ┆     ┆     ┆ 38  ┆ 091 ┆ 147 ┆ 288 ┆ 042 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 74  ┆ 35  ┆ 11  ┆     ┆ 15  ┆     ┆     ┆ 03  ┆     ┆ 125 ┆ 22  ┆     ┆ 01  ┆ 61  ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 595 ┆ 09  ┆ 929 ┆ 602 ┆ 15  ┆ 54  ┆     ┆     ┆ 891 ┆ 412 ┆ 522 ┆ 244 ┆ 37  │\n",
       "│ 0   ┆ 0   ┆ 14  ┆ 0.4 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.9 ┆ 0.2 ┆ 0.3 ┆ -0. ┆ 44  ┆ 3   ┆ 16  ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ 0.6 ┆ nul ┆ -1. ┆ -1. ┆ -0. ┆ nul ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ nul ┆ nul ┆ 0.0 ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -2. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -2. ┆ -2. ┆ -3. ┆ nul ┆ 1.2 ┆ nul ┆ nul ┆ 0.4 ┆ nul ┆ -0. ┆ 2.8 ┆ nul ┆ 1.3 ┆ 0.4 ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -1. ┆ 0.0 ┆ -0. ┆ nul ┆ nul ┆ 3.6 ┆ 2.7 ┆ 2.6 ┆ 3.4 ┆ -3. │\n",
       "│     ┆     ┆     ┆ 405 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 552 ┆ 624 ┆ 444 ┆ 613 ┆     ┆     ┆     ┆ 947 ┆ 030 ┆ 502 ┆ l   ┆ 460 ┆ l   ┆ 844 ┆ 586 ┆ 182 ┆ l   ┆ 969 ┆ 673 ┆ 282 ┆ 399 ┆ l   ┆ l   ┆ 438 ┆ 320 ┆ 031 ┆ l   ┆ l   ┆ l   ┆ 088 ┆ 995 ┆ 635 ┆ 196 ┆ 618 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 014 ┆ 321 ┆ 711 ┆ l   ┆ 539 ┆ l   ┆ l   ┆ 761 ┆ l   ┆ 771 ┆ 434 ┆ l   ┆ 798 ┆ 118 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 948 ┆ 136 ┆ 447 ┆ 141 ┆ 996 ┆ 661 ┆ l   ┆ l   ┆ 780 ┆ 935 ┆ 182 ┆ 181 ┆ 572 │\n",
       "│     ┆     ┆     ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆ 04  ┆ 57  ┆ 813 ┆     ┆     ┆     ┆ 351 ┆ 018 ┆ 379 ┆     ┆ 86  ┆     ┆ 685 ┆ 56  ┆ 024 ┆     ┆ 949 ┆ 813 ┆ 132 ┆ 894 ┆     ┆     ┆ 15  ┆ 225 ┆ 713 ┆     ┆     ┆     ┆ 42  ┆ 003 ┆ 336 ┆ 461 ┆ 719 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 6   ┆ 076 ┆ 265 ┆     ┆ 02  ┆     ┆     ┆ 95  ┆     ┆ 732 ┆ 21  ┆     ┆ 15  ┆ 27  ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 601 ┆ 814 ┆ 704 ┆ 761 ┆ 31  ┆ 928 ┆     ┆     ┆ 76  ┆ 81  ┆ 5   ┆ 33  ┆ 82  │\n",
       "│ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   ┆ …   │\n",
       "│ 169 ┆ 967 ┆ 34  ┆ 3.2 ┆ 2.5 ┆ -0. ┆ 2.5 ┆ 2.4 ┆ 0.4 ┆ 0.7 ┆ 1.1 ┆ 2.1 ┆ 0.4 ┆ 42  ┆ 5   ┆ 150 ┆ 0.8 ┆ 1.1 ┆ 1.0 ┆ -0. ┆ -0. ┆ -0. ┆ 1.7 ┆ -0. ┆ -0. ┆ -0. ┆ 0.9 ┆ 1.3 ┆ -0. ┆ -0. ┆ -0. ┆ 1.0 ┆ 1.4 ┆ -0. ┆ -1. ┆ -0. ┆ 3.4 ┆ -0. ┆ 3.1 ┆ 3.4 ┆ -1. ┆ -0. ┆ -0. ┆ 2.4 ┆ 0.9 ┆ 1.1 ┆ 0.1 ┆ -0. ┆ 0.0 ┆ 1.3 ┆ 0.8 ┆ 1.5 ┆ 1.0 ┆ 1.2 ┆ 2.5 ┆ 0.0 ┆ 0.9 ┆ 1.3 ┆ -0. ┆ 0.5 ┆ 1.1 ┆ 1.1 ┆ 2.2 ┆ 0.6 ┆ 2.4 ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ 2.4 ┆ -0. ┆ 0.7 ┆ 0.8 ┆ 0.3 ┆ 1.3 ┆ 1.0 ┆ 1.7 ┆ -0. ┆ -0. ┆ 0.2 ┆ 0.3 ┆ -0. ┆ 0.0 ┆ -0. │\n",
       "│ 8   ┆     ┆     ┆ 424 ┆ 251 ┆ 721 ┆ 440 ┆ 776 ┆ 175 ┆ 858 ┆ 177 ┆ 994 ┆ 154 ┆     ┆     ┆     ┆ 044 ┆ 572 ┆ 315 ┆ 671 ┆ 328 ┆ 486 ┆ 301 ┆ 006 ┆ 001 ┆ 213 ┆ 326 ┆ 673 ┆ 238 ┆ 692 ┆ 121 ┆ 907 ┆ 442 ┆ 675 ┆ 013 ┆ 242 ┆ 276 ┆ 958 ┆ 398 ┆ 162 ┆ 655 ┆ 599 ┆ 932 ┆ 934 ┆ 694 ┆ 020 ┆ 589 ┆ 496 ┆ 361 ┆ 098 ┆ 280 ┆ 779 ┆ 408 ┆ 553 ┆ 774 ┆ 574 ┆ 530 ┆ 770 ┆ 396 ┆ 202 ┆ 796 ┆ 276 ┆ 319 ┆ 146 ┆ 128 ┆ 101 ┆ 384 ┆ 275 ┆ 408 ┆ 271 ┆ 108 ┆ 397 ┆ 302 ┆ 662 ┆ 332 ┆ 754 ┆ 982 ┆ 183 ┆ 190 ┆ 342 ┆ 471 ┆ 044 ┆ 169 ┆ 132 │\n",
       "│     ┆     ┆     ┆ 93  ┆ 6   ┆ 981 ┆ 25  ┆ 15  ┆ 57  ┆ 12  ┆ 96  ┆ 36  ┆ 27  ┆     ┆     ┆     ┆ 03  ┆ 57  ┆ 43  ┆ 189 ┆ 6   ┆ 132 ┆ 76  ┆ 173 ┆ 144 ┆ 062 ┆ 18  ┆ 38  ┆ 197 ┆ 615 ┆ 163 ┆ 98  ┆ 94  ┆ 626 ┆ 264 ┆ 888 ┆ 39  ┆ 278 ┆ 36  ┆ 78  ┆ 316 ┆ 44  ┆ 876 ┆ 58  ┆ 62  ┆ 16  ┆ 82  ┆ 177 ┆ 77  ┆ 66  ┆ 25  ┆ 55  ┆ 02  ┆ 98  ┆ 41  ┆ 55  ┆ 05  ┆ 51  ┆ 358 ┆ 62  ┆ 17  ┆ 57  ┆ 28  ┆ 52  ┆ 86  ┆ 531 ┆ 833 ┆ 818 ┆ 04  ┆ 15  ┆ 427 ┆ 34  ┆ 05  ┆ 87  ┆ 5   ┆ 99  ┆ 64  ┆ 443 ┆ 222 ┆ 11  ┆ 42  ┆ 463 ┆ 36  ┆ 337 │\n",
       "│ 169 ┆ 967 ┆ 35  ┆ 1.0 ┆ 1.8 ┆ -0. ┆ 2.7 ┆ 2.3 ┆ 0.8 ┆ 0.6 ┆ 1.1 ┆ 1.9 ┆ 0.3 ┆ 25  ┆ 7   ┆ 195 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 0.2 ┆ -0. ┆ 1.8 ┆ 0.1 ┆ -0. ┆ -0. ┆ -1. ┆ -0. ┆ 0.4 ┆ 0.0 ┆ 0.1 ┆ -0. ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ 3.8 ┆ -1. ┆ 3.4 ┆ 2.8 ┆ 0.7 ┆ 0.0 ┆ -0. ┆ 0.6 ┆ 0.1 ┆ 0.6 ┆ 0.0 ┆ -0. ┆ 0.0 ┆ 0.4 ┆ -0. ┆ -0. ┆ 0.9 ┆ 0.8 ┆ 0.8 ┆ 0.2 ┆ -0. ┆ 0.6 ┆ -1. ┆ 0.1 ┆ -0. ┆ -1. ┆ -0. ┆ 0.2 ┆ 0.2 ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ 2.0 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 1.0 ┆ 0.7 ┆ 0.0 ┆ 0.0 ┆ -0. │\n",
       "│ 8   ┆     ┆     ┆ 791 ┆ 579 ┆ 790 ┆ 454 ┆ 398 ┆ 450 ┆ 513 ┆ 803 ┆ 663 ┆ 215 ┆     ┆     ┆     ┆ 075 ┆ 152 ┆ 204 ┆ 421 ┆ 170 ┆ 258 ┆ 749 ┆ 998 ┆ 199 ┆ 125 ┆ 004 ┆ 051 ┆ 509 ┆ 092 ┆ 641 ┆ 939 ┆ 143 ┆ 320 ┆ 379 ┆ 142 ┆ 624 ┆ 451 ┆ 774 ┆ 616 ┆ 634 ┆ 759 ┆ 119 ┆ 260 ┆ 488 ┆ 532 ┆ 593 ┆ 845 ┆ 985 ┆ 095 ┆ 675 ┆ 011 ┆ 305 ┆ 319 ┆ 089 ┆ 192 ┆ 315 ┆ 877 ┆ 189 ┆ 801 ┆ 175 ┆ 604 ┆ 209 ┆ 498 ┆ 888 ┆ 101 ┆ 343 ┆ 253 ┆ 278 ┆ 506 ┆ 059 ┆ 029 ┆ 101 ┆ 187 ┆ 180 ┆ 086 ┆ 153 ┆ 196 ┆ 175 ┆ 457 ┆ 397 ┆ 337 ┆ 508 ┆ 249 │\n",
       "│     ┆     ┆     ┆ 39  ┆ 06  ┆ 646 ┆ 39  ┆ 77  ┆ 65  ┆ 7   ┆ 01  ┆ 79  ┆ 43  ┆     ┆     ┆     ┆ 294 ┆ 726 ┆ 17  ┆ 137 ┆ 8   ┆ 775 ┆ 78  ┆ 8   ┆ 219 ┆ 619 ┆ 547 ┆ 933 ┆ 05  ┆ 46  ┆ 27  ┆ 974 ┆ 421 ┆ 071 ┆ 835 ┆ 429 ┆ 69  ┆ 786 ┆ 89  ┆ 63  ┆ 59  ┆ 72  ┆ 677 ┆ 35  ┆ 15  ┆ 81  ┆ 13  ┆ 099 ┆ 28  ┆ 64  ┆ 728 ┆ 334 ┆ 34  ┆ 8   ┆ 55  ┆ 76  ┆ 776 ┆ 55  ┆ 577 ┆ 46  ┆ 486 ┆ 35  ┆ 283 ┆ 47  ┆ 16  ┆ 531 ┆ 868 ┆ 991 ┆ 832 ┆ 39  ┆ 506 ┆ 396 ┆ 381 ┆ 759 ┆ 839 ┆ 1   ┆ 405 ┆ 077 ┆ 292 ┆ 8   ┆ 33  ┆ 2   ┆ 6   ┆ 584 │\n",
       "│ 169 ┆ 967 ┆ 36  ┆ 1.0 ┆ 2.5 ┆ -0. ┆ 2.2 ┆ 2.5 ┆ 0.2 ┆ 0.9 ┆ 1.1 ┆ 2.1 ┆ 0.2 ┆ 49  ┆ 7   ┆ 297 ┆ 1.0 ┆ -0. ┆ 0.2 ┆ -0. ┆ -0. ┆ -0. ┆ 2.3 ┆ 0.3 ┆ -0. ┆ nul ┆ -0. ┆ -0. ┆ 0.2 ┆ -0. ┆ nul ┆ nul ┆ -1. ┆ -0. ┆ -0. ┆ nul ┆ 2.3 ┆ -0. ┆ 3.2 ┆ 3.0 ┆ 0.4 ┆ 0.2 ┆ -0. ┆ 1.0 ┆ -0. ┆ 2.0 ┆ 0.4 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 0.0 ┆ 0.0 ┆ 1.7 ┆ 0.7 ┆ -0. ┆ 1.3 ┆ 0.9 ┆ 0.0 ┆ -0. ┆ -0. ┆ -0. ┆ 0.1 ┆ 0.1 ┆ 1.8 ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ 2.2 ┆ 0.6 ┆ 1.0 ┆ -0. ┆ 0.1 ┆ 0.8 ┆ 0.0 ┆ 0.3 ┆ -0. ┆ -0. ┆ 0.0 ┆ 0.0 ┆ 0.1 ┆ 0.1 ┆ -0. │\n",
       "│ 8   ┆     ┆     ┆ 331 ┆ 155 ┆ 672 ┆ 892 ┆ 215 ┆ 550 ┆ 198 ┆ 720 ┆ 804 ┆ 484 ┆     ┆     ┆     ┆ 267 ┆ 096 ┆ 243 ┆ 528 ┆ 704 ┆ 704 ┆ 124 ┆ 280 ┆ 108 ┆ l   ┆ 945 ┆ 244 ┆ 059 ┆ 357 ┆ l   ┆ l   ┆ 110 ┆ 580 ┆ 400 ┆ l   ┆ 978 ┆ 637 ┆ 606 ┆ 467 ┆ 409 ┆ 348 ┆ 175 ┆ 224 ┆ 500 ┆ 710 ┆ 134 ┆ 450 ┆ 156 ┆ 253 ┆ 769 ┆ 660 ┆ 478 ┆ 137 ┆ 727 ┆ 549 ┆ 384 ┆ 335 ┆ 329 ┆ 519 ┆ 290 ┆ 806 ┆ 062 ┆ 834 ┆ 304 ┆ 101 ┆ 341 ┆ 249 ┆ 343 ┆ 513 ┆ 018 ┆ 350 ┆ 283 ┆ 072 ┆ 601 ┆ 242 ┆ 748 ┆ 220 ┆ 161 ┆ 327 ┆ 368 ┆ 689 ┆ 523 ┆ 065 │\n",
       "│     ┆     ┆     ┆ 72  ┆ 27  ┆ 298 ┆ 5   ┆ 92  ┆ 77  ┆ 92  ┆ 18  ┆ 96  ┆ 6   ┆     ┆     ┆     ┆ 15  ┆ 892 ┆ 09  ┆ 109 ┆ 952 ┆ 818 ┆ 82  ┆ 4   ┆ 193 ┆     ┆ 684 ┆ 173 ┆ 89  ┆ 343 ┆     ┆     ┆ 75  ┆ 242 ┆ 568 ┆     ┆ 77  ┆ 258 ┆ 38  ┆ 86  ┆ 65  ┆ 42  ┆ 58  ┆ 06  ┆ 069 ┆ 33  ┆ 88  ┆ 016 ┆ 616 ┆ 755 ┆ 588 ┆ 86  ┆ 26  ┆ 07  ┆ 72  ┆ 192 ┆ 74  ┆ 68  ┆ 78  ┆ 118 ┆ 343 ┆ 786 ┆ 95  ┆ 61  ┆ 21  ┆ 531 ┆ 991 ┆ 132 ┆ 65  ┆ 58  ┆ 88  ┆ 51  ┆ 241 ┆ 44  ┆ 6   ┆ 23  ┆ 52  ┆ 933 ┆ 584 ┆ 71  ┆ 88  ┆ 08  ┆ 33  ┆ 355 │\n",
       "│ 169 ┆ 967 ┆ 37  ┆ 1.2 ┆ 2.6 ┆ -0. ┆ 2.3 ┆ 3.1 ┆ 0.3 ┆ 0.6 ┆ 1.1 ┆ 1.5 ┆ 0.3 ┆ 34  ┆ 4   ┆ 214 ┆ 0.7 ┆ 0.2 ┆ 0.4 ┆ -0. ┆ -0. ┆ -0. ┆ 1.8 ┆ 0.4 ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ 0.0 ┆ -0. ┆ 0.1 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 3.0 ┆ -0. ┆ 3.1 ┆ 3.0 ┆ 0.8 ┆ 0.0 ┆ -0. ┆ 0.5 ┆ 0.0 ┆ 0.3 ┆ 0.6 ┆ -0. ┆ -0. ┆ 0.2 ┆ -0. ┆ 0.5 ┆ -0. ┆ 0.6 ┆ 0.9 ┆ 0.3 ┆ 0.4 ┆ 1.8 ┆ -0. ┆ 0.8 ┆ 0.3 ┆ -0. ┆ 0.7 ┆ -0. ┆ 1.2 ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ 2.4 ┆ 0.5 ┆ 0.9 ┆ -0. ┆ 0.1 ┆ 0.4 ┆ 0.7 ┆ 0.5 ┆ -0. ┆ -0. ┆ 0.1 ┆ 0.1 ┆ -0. ┆ -0. ┆ -0. │\n",
       "│ 8   ┆     ┆     ┆ 431 ┆ 632 ┆ 889 ┆ 131 ┆ 014 ┆ 244 ┆ 189 ┆ 856 ┆ 997 ┆ 197 ┆     ┆     ┆     ┆ 593 ┆ 840 ┆ 171 ┆ 611 ┆ 513 ┆ 891 ┆ 499 ┆ 067 ┆ 608 ┆ 252 ┆ 271 ┆ 051 ┆ 981 ┆ 653 ┆ 736 ┆ 016 ┆ 404 ┆ 577 ┆ 731 ┆ 216 ┆ 185 ┆ 472 ┆ 392 ┆ 658 ┆ 429 ┆ 532 ┆ 074 ┆ 001 ┆ 826 ┆ 362 ┆ 439 ┆ 422 ┆ 418 ┆ 030 ┆ 702 ┆ 433 ┆ 195 ┆ 933 ┆ 532 ┆ 525 ┆ 717 ┆ 764 ┆ 143 ┆ 455 ┆ 011 ┆ 395 ┆ 380 ┆ 041 ┆ 706 ┆ 101 ┆ 358 ┆ 141 ┆ 255 ┆ 892 ┆ 376 ┆ 821 ┆ 158 ┆ 373 ┆ 783 ┆ 826 ┆ 814 ┆ 106 ┆ 111 ┆ 638 ┆ 693 ┆ 037 ┆ 029 ┆ 148 │\n",
       "│     ┆     ┆     ┆ 16  ┆ 98  ┆ 112 ┆ 55  ┆ 28  ┆ 54  ┆ 44  ┆ 63  ┆ 24  ┆ 19  ┆     ┆     ┆     ┆ 14  ┆ 57  ┆ 6   ┆ 075 ┆ 717 ┆ 423 ┆ 4   ┆ 56  ┆ 196 ┆ 663 ┆ 574 ┆ 405 ┆ 46  ┆ 961 ┆ 76  ┆ 497 ┆ 509 ┆ 262 ┆ 429 ┆ 46  ┆ 64  ┆ 061 ┆ 2   ┆ 58  ┆ 25  ┆ 83  ┆ 403 ┆ 29  ┆ 3   ┆ 23  ┆ 34  ┆ 367 ┆ 195 ┆ 37  ┆ 278 ┆ 05  ┆ 764 ┆ 64  ┆ 93  ┆ 67  ┆ 75  ┆ 59  ┆ 377 ┆ 16  ┆ 35  ┆ 703 ┆ 38  ┆ 24  ┆ 45  ┆ 531 ┆ 106 ┆ 883 ┆ 192 ┆ 47  ┆ 52  ┆ 07  ┆ 009 ┆ 89  ┆ 57  ┆ 92  ┆ 21  ┆ 056 ┆ 017 ┆ 67  ┆ 31  ┆ 563 ┆ 483 ┆ 711 │\n",
       "│ 169 ┆ 967 ┆ 38  ┆ 3.1 ┆ 2.7 ┆ -0. ┆ 2.7 ┆ 2.3 ┆ 0.4 ┆ 0.8 ┆ 0.9 ┆ 2.0 ┆ 0.3 ┆ 50  ┆ 1   ┆ 522 ┆ 0.4 ┆ 0.6 ┆ 1.0 ┆ -0. ┆ -0. ┆ -0. ┆ 0.3 ┆ 1.9 ┆ -0. ┆ -0. ┆ 1.7 ┆ 1.3 ┆ -0. ┆ -0. ┆ 0.5 ┆ 0.5 ┆ 0.2 ┆ -0. ┆ -0. ┆ -0. ┆ 3.6 ┆ -0. ┆ 3.5 ┆ 3.1 ┆ -1. ┆ -0. ┆ -0. ┆ 1.3 ┆ -0. ┆ 0.0 ┆ 1.0 ┆ -0. ┆ -0. ┆ 0.5 ┆ 0.8 ┆ 0.5 ┆ -0. ┆ 0.4 ┆ 1.2 ┆ 0.2 ┆ 0.9 ┆ 1.8 ┆ 0.5 ┆ 1.3 ┆ 0.6 ┆ 1.6 ┆ 1.0 ┆ 0.5 ┆ 0.6 ┆ -1. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 2.0 ┆ 0.2 ┆ 0.2 ┆ 0.2 ┆ 0.4 ┆ 0.7 ┆ 0.7 ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. │\n",
       "│ 8   ┆     ┆     ┆ 936 ┆ 285 ┆ 745 ┆ 887 ┆ 433 ┆ 547 ┆ 628 ┆ 647 ┆ 896 ┆ 449 ┆     ┆     ┆     ┆ 065 ┆ 182 ┆ 132 ┆ 952 ┆ 679 ┆ 597 ┆ 751 ┆ 753 ┆ 440 ┆ 072 ┆ 413 ┆ 807 ┆ 110 ┆ 874 ┆ 534 ┆ 322 ┆ 632 ┆ 757 ┆ 869 ┆ 062 ┆ 192 ┆ 386 ┆ 445 ┆ 206 ┆ 443 ┆ 257 ┆ 309 ┆ 663 ┆ 220 ┆ 297 ┆ 944 ┆ 051 ┆ 114 ┆ 173 ┆ 522 ┆ 221 ┆ 027 ┆ 715 ┆ 131 ┆ 632 ┆ 158 ┆ 620 ┆ 038 ┆ 101 ┆ 625 ┆ 549 ┆ 903 ┆ 359 ┆ 530 ┆ 101 ┆ 622 ┆ 363 ┆ 395 ┆ 016 ┆ 167 ┆ 414 ┆ 532 ┆ 287 ┆ 627 ┆ 996 ┆ 061 ┆ 376 ┆ 286 ┆ 359 ┆ 246 ┆ 288 ┆ 247 ┆ 138 │\n",
       "│     ┆     ┆     ┆ 85  ┆ 06  ┆ 238 ┆ 89  ┆ 93  ┆ 31  ┆ 39  ┆ 95  ┆ 73  ┆ 31  ┆     ┆     ┆     ┆ 31  ┆ 47  ┆ 7   ┆ 069 ┆ 168 ┆ 603 ┆ 25  ┆ 7   ┆ 974 ┆ 018 ┆ 53  ┆ 35  ┆ 494 ┆ 806 ┆ 24  ┆ 43  ┆ 14  ┆ 856 ┆ 204 ┆ 955 ┆ 33  ┆ 316 ┆ 6   ┆ 31  ┆ 649 ┆ 411 ┆ 567 ┆ 58  ┆ 885 ┆ 98  ┆ 89  ┆ 078 ┆ 243 ┆ 13  ┆ 01  ┆ 99  ┆ 275 ┆ 93  ┆ 11  ┆ 78  ┆ 04  ┆ 22  ┆ 19  ┆ 26  ┆ 21  ┆ 48  ┆ 67  ┆ 22  ┆ 11  ┆ 531 ┆ 853 ┆ 631 ┆ 652 ┆ 812 ┆ 34  ┆ 86  ┆ 29  ┆ 45  ┆ 17  ┆ 35  ┆ 02  ┆ 377 ┆ 764 ┆ 046 ┆ 135 ┆ 941 ┆ 774 ┆ 548 │\n",
       "└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pl.read_parquet(path + 'train.parquet/').drop(['responder_0', 'responder_1', 'responder_2', 'responder_3', 'responder_4', 'responder_5', 'responder_7', 'responder_8', 'partition_id']).select(pl.all().shrink_dtype())\n",
    "print(train_df.shape)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3834f8e8-0f10-4891-a325-5d288f99b503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.282841602"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.estimated_size() / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3ffe03-76cd-4ee5-bf7c-940de4646a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = path + 'my_folder/models/20250110_03/'\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38a6217-e920-4eb5-b270-25bd59fa1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_models_path = path + 'my_folder/models/20250109_03/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63cd0720-e4db-47f5-baa2-6e4a24eedb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_window_size</th>\n",
       "      <th>training_window_size</th>\n",
       "      <th>fraction</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_data_in_leaf</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>min_gain_to_split</th>\n",
       "      <th>lambda_l1</th>\n",
       "      <th>lambda_l2</th>\n",
       "      <th>feature_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>618</td>\n",
       "      <td>0.152297</td>\n",
       "      <td>0.04979</td>\n",
       "      <td>18</td>\n",
       "      <td>102</td>\n",
       "      <td>8995</td>\n",
       "      <td>0.237934</td>\n",
       "      <td>7.797875</td>\n",
       "      <td>1046.995871</td>\n",
       "      <td>0.94769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   val_window_size  training_window_size  fraction  learning_rate  max_depth  \\\n",
       "0               60                   618  0.152297        0.04979         18   \n",
       "\n",
       "   min_data_in_leaf  num_leaves  min_gain_to_split  lambda_l1    lambda_l2  \\\n",
       "0               102        8995           0.237934   7.797875  1046.995871   \n",
       "\n",
       "   feature_fraction  \n",
       "0           0.94769  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params_df = pd.read_csv(previous_models_path + 'lgb_params.csv')\n",
    "lgb_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e04716-a5a1-471c-b452-0a8b5f7a7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_sliding_window(train_data):#, optuna_n_trials):\n",
    "\n",
    "    unique_date_ids = sorted(train_data['date_id'].unique())\n",
    "    #unique_date_ids = [i for i in range(1110, train_data['date_id'].max()+1)]\n",
    "    #date_ids_df = train_data['date_id'].to_frame()\n",
    "    sample_df = train_data.sample(fraction=lgb_params_df['fraction'][0])\n",
    "    unique_date_ids = sorted(sample_df['date_id'].unique())\n",
    "    print(len(unique_date_ids))\n",
    "\n",
    "    for date_id in unique_date_ids:\n",
    "        val_window_size = lgb_params_df['val_window_size'][0]\n",
    "        training_window_size = lgb_params_df['training_window_size'][0]\n",
    "        #fraction = trial.suggest_float('fraction', 0.05, 0.3)\n",
    "        #sample_df = train_data.sample(fraction=fraction)\n",
    "        #unique_date_ids = [i for i in range(1110, sample_df['date_id'].max()+1)]\n",
    "        #date_id = random.choice(unique_date_ids)\n",
    "\n",
    "        test_date_id_df = sample_df.filter(pl.col('date_id') == date_id)\n",
    "\n",
    "        val_date_id_cut_lower = date_id - val_window_size\n",
    "        if val_date_id_cut_lower < 0:\n",
    "            val_window_df = sample_df.filter(pl.col('date_id') <= val_window_size)\n",
    "            val_window_df = val_window_df.join(test_date_id_df, on=['date_id', 'time_id', 'symbol_id'], how='anti')\n",
    "        else:\n",
    "            val_window_df = sample_df.filter((pl.col('date_id') >= val_date_id_cut_lower)&(pl.col('date_id') < date_id))\n",
    "        \n",
    "        training_date_id_cut_lower = val_date_id_cut_lower - training_window_size\n",
    "        if training_date_id_cut_lower < 0:\n",
    "            training_window_df = sample_df.filter(pl.col('date_id') <= val_window_size + training_window_size)\n",
    "            training_window_df = training_window_df.join(test_date_id_df, on=['date_id', 'time_id', 'symbol_id'], how='anti')\n",
    "            training_window_df = training_window_df.join(val_window_df, on=['date_id', 'time_id', 'symbol_id'], how='anti')\n",
    "        else:\n",
    "            training_window_df = sample_df.filter((pl.col('date_id') >= training_date_id_cut_lower)&(pl.col('date_id') < val_date_id_cut_lower))\n",
    "\n",
    "        \n",
    "        '''\n",
    "        test_date_id_df = sample_df.filter(pl.col('date_id') == date_id)\n",
    "\n",
    "        val_date_id_cut_lower = date_id - val_window_size\n",
    "        val_window_df = sample_df.filter((pl.col('date_id') >= val_date_id_cut_lower)&(pl.col('date_id') < date_id))#.sample(fraction=fraction)\n",
    "\n",
    "        training_date_id_cut_lower = val_date_id_cut_lower - training_window_size\n",
    "        training_window_df = sample_df.filter((pl.col('date_id') >= training_date_id_cut_lower)&(pl.col('date_id') < val_date_id_cut_lower))#.sample(fraction=fraction).sort(by=['date_id', 'time_id', 'symbol_id'])\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        print(date_id)\n",
    "        print('this is training_window_df')\n",
    "        display(training_window_df)\n",
    "        print('training_window_size:', training_window_size)\n",
    "        print('n unique in training_window_df:', training_window_df['date_id'].n_unique())\n",
    "\n",
    "        print('this is val_window_df')\n",
    "        display(val_window_df)\n",
    "        print('val_window_size:', val_window_size)\n",
    "        print('n unique in training_window_df:', val_window_df['date_id'].n_unique())\n",
    "\n",
    "        print('this is test_date_id_df')\n",
    "        display(test_date_id_df)\n",
    "        print('n unique in test_date_id_df:', test_date_id_df['date_id'].n_unique())\n",
    "        '''\n",
    "\n",
    "        #training_window_df = training_window_df.sample(fraction=fraction)\n",
    "                \n",
    "\n",
    "        '''\n",
    "        print('this is window_df')\n",
    "        display(window_df)\n",
    "        print('this is date_id_df')\n",
    "        display(date_id_df)\n",
    "        '''\n",
    "    \n",
    "        base_params = {\n",
    "            'verbosity': -1,\n",
    "            'device': 'gpu',\n",
    "            'early_stopping_round': 20,\n",
    "        }\n",
    "    \n",
    "        params_to_tune = {\n",
    "            'learning_rate': lgb_params_df['learning_rate'][0],\n",
    "            'max_depth': lgb_params_df['max_depth'][0],\n",
    "            'min_data_in_leaf': lgb_params_df['min_data_in_leaf'][0],\n",
    "            'num_leaves': lgb_params_df['num_leaves'][0],\n",
    "            'min_gain_to_split': lgb_params_df['min_gain_to_split'][0],\n",
    "            'lambda_l1': lgb_params_df['lambda_l1'][0],\n",
    "            'lambda_l2': lgb_params_df['lambda_l2'][0],\n",
    "            'feature_fraction': lgb_params_df['feature_fraction'][0],\n",
    "        }\n",
    "    \n",
    "        model = LGBMRegressor(\n",
    "            **base_params,\n",
    "            **params_to_tune,\n",
    "            n_estimators=100000\n",
    "        )\n",
    "    \n",
    "        X_train = training_window_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "        X_val = val_window_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "        X_test = test_date_id_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "    \n",
    "        y_train = training_window_df['responder_6'].to_pandas()\n",
    "        y_val = val_window_df['responder_6'].to_pandas()\n",
    "        y_test = test_date_id_df['responder_6'].to_pandas()\n",
    "    \n",
    "        weights_train = training_window_df['weight'].to_pandas()\n",
    "        weights_val = val_window_df['weight'].to_pandas()\n",
    "        weights_test = test_date_id_df['weight'].to_pandas()\n",
    "    \n",
    "        model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val])#, callbacks=[log_evaluation(period=10)])\n",
    "        \n",
    "        test_preds = model.predict(X_test)\n",
    "\n",
    "        test_score = r2_score(y_test, test_preds, sample_weight=weights_test)\n",
    "\n",
    "        print('date_id is:', date_id)\n",
    "        print('Test Weighted R2 score is:', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd82087-86f0-4bb0-9054-6b8a249055ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699\n",
      "date_id is: 0\n",
      "Test Weighted R2 score is: 0.014591922812926428\n",
      "date_id is: 1\n",
      "Test Weighted R2 score is: 0.012558011718474815\n",
      "date_id is: 2\n",
      "Test Weighted R2 score is: 0.00827400003005685\n",
      "date_id is: 3\n",
      "Test Weighted R2 score is: -0.007121512215316539\n",
      "date_id is: 4\n",
      "Test Weighted R2 score is: 0.009612025733562679\n",
      "date_id is: 5\n",
      "Test Weighted R2 score is: 0.026076773062774672\n",
      "date_id is: 6\n",
      "Test Weighted R2 score is: 0.034359993296513514\n",
      "date_id is: 7\n",
      "Test Weighted R2 score is: 0.01338002137788774\n",
      "date_id is: 8\n",
      "Test Weighted R2 score is: -0.021289405634935754\n",
      "date_id is: 9\n",
      "Test Weighted R2 score is: -0.0012550990513253701\n",
      "date_id is: 10\n",
      "Test Weighted R2 score is: 0.02012594276418045\n",
      "date_id is: 11\n",
      "Test Weighted R2 score is: 0.0007980184400396828\n",
      "date_id is: 12\n",
      "Test Weighted R2 score is: 0.022403686994295624\n",
      "date_id is: 13\n",
      "Test Weighted R2 score is: 0.024580105040958644\n",
      "date_id is: 14\n",
      "Test Weighted R2 score is: 0.04254823368834115\n",
      "date_id is: 15\n",
      "Test Weighted R2 score is: 0.022147557404272322\n",
      "date_id is: 16\n",
      "Test Weighted R2 score is: 0.006576293098923491\n",
      "date_id is: 17\n",
      "Test Weighted R2 score is: 0.011954438954546287\n",
      "date_id is: 18\n",
      "Test Weighted R2 score is: 0.01531862675212703\n",
      "date_id is: 19\n",
      "Test Weighted R2 score is: 0.028938615954611557\n",
      "date_id is: 20\n",
      "Test Weighted R2 score is: -0.03243093624622273\n",
      "date_id is: 21\n",
      "Test Weighted R2 score is: 0.010178427165910664\n",
      "date_id is: 22\n",
      "Test Weighted R2 score is: 0.016810349256583867\n",
      "date_id is: 23\n",
      "Test Weighted R2 score is: 0.011972821512692855\n",
      "date_id is: 24\n",
      "Test Weighted R2 score is: 0.008611364577356229\n",
      "date_id is: 25\n",
      "Test Weighted R2 score is: 0.03174198742460477\n",
      "date_id is: 26\n",
      "Test Weighted R2 score is: -0.08382192527132992\n",
      "date_id is: 27\n",
      "Test Weighted R2 score is: 0.012000976778983596\n",
      "date_id is: 28\n",
      "Test Weighted R2 score is: 0.01188378173797544\n",
      "date_id is: 29\n",
      "Test Weighted R2 score is: 0.02654180196438216\n",
      "date_id is: 30\n",
      "Test Weighted R2 score is: -0.007824713160129226\n",
      "date_id is: 31\n",
      "Test Weighted R2 score is: 0.0013410689378204887\n",
      "date_id is: 32\n",
      "Test Weighted R2 score is: -0.003905664163562861\n",
      "date_id is: 33\n",
      "Test Weighted R2 score is: 0.0025332056937652503\n",
      "date_id is: 34\n",
      "Test Weighted R2 score is: -0.004536796823854772\n",
      "date_id is: 35\n",
      "Test Weighted R2 score is: 0.0032630259957887597\n",
      "date_id is: 36\n",
      "Test Weighted R2 score is: 0.02154822230035014\n",
      "date_id is: 37\n",
      "Test Weighted R2 score is: 0.039552359410890636\n",
      "date_id is: 38\n",
      "Test Weighted R2 score is: 0.0067111914044387655\n",
      "date_id is: 39\n",
      "Test Weighted R2 score is: -0.005657703839160755\n",
      "date_id is: 40\n",
      "Test Weighted R2 score is: 0.02075773733004349\n",
      "date_id is: 41\n",
      "Test Weighted R2 score is: 0.0021138457267445565\n",
      "date_id is: 42\n",
      "Test Weighted R2 score is: -0.01886384378720596\n",
      "date_id is: 43\n",
      "Test Weighted R2 score is: -0.01212352032735553\n",
      "date_id is: 44\n",
      "Test Weighted R2 score is: 0.010126799978255163\n",
      "date_id is: 45\n",
      "Test Weighted R2 score is: 0.0076973339734314195\n",
      "date_id is: 46\n",
      "Test Weighted R2 score is: -0.011457759753575658\n",
      "date_id is: 47\n",
      "Test Weighted R2 score is: 0.027549333164392564\n",
      "date_id is: 48\n",
      "Test Weighted R2 score is: 0.022588802027701083\n",
      "date_id is: 49\n",
      "Test Weighted R2 score is: 0.02078597410713623\n",
      "date_id is: 50\n",
      "Test Weighted R2 score is: 0.012273280621041383\n",
      "date_id is: 51\n",
      "Test Weighted R2 score is: 0.012050369187360244\n",
      "date_id is: 52\n",
      "Test Weighted R2 score is: 0.007293531382181917\n",
      "date_id is: 53\n",
      "Test Weighted R2 score is: 0.014865946356219628\n",
      "date_id is: 54\n",
      "Test Weighted R2 score is: 0.018389008679750574\n",
      "date_id is: 55\n",
      "Test Weighted R2 score is: 0.015384962882204478\n",
      "date_id is: 56\n",
      "Test Weighted R2 score is: 0.012184168367054937\n",
      "date_id is: 57\n",
      "Test Weighted R2 score is: -0.002434200092502392\n",
      "date_id is: 58\n",
      "Test Weighted R2 score is: 0.006363878332677042\n",
      "date_id is: 59\n",
      "Test Weighted R2 score is: 0.04639863929329102\n",
      "date_id is: 60\n",
      "Test Weighted R2 score is: 0.00414326325159764\n",
      "date_id is: 61\n",
      "Test Weighted R2 score is: 0.027597470618253017\n",
      "date_id is: 62\n",
      "Test Weighted R2 score is: 0.025646747191827246\n",
      "date_id is: 63\n",
      "Test Weighted R2 score is: 0.01232302165448318\n",
      "date_id is: 64\n",
      "Test Weighted R2 score is: 0.02178699398423778\n",
      "date_id is: 65\n",
      "Test Weighted R2 score is: 0.010514456198354782\n",
      "date_id is: 66\n",
      "Test Weighted R2 score is: 0.00223131237631069\n",
      "date_id is: 67\n",
      "Test Weighted R2 score is: -0.009884301842986387\n",
      "date_id is: 68\n",
      "Test Weighted R2 score is: 0.016826069360152407\n",
      "date_id is: 69\n",
      "Test Weighted R2 score is: 0.000371760958926437\n",
      "date_id is: 70\n",
      "Test Weighted R2 score is: 0.023405174838324716\n",
      "date_id is: 71\n",
      "Test Weighted R2 score is: 0.0016757939585668735\n",
      "date_id is: 72\n",
      "Test Weighted R2 score is: 0.030733168470812866\n",
      "date_id is: 73\n",
      "Test Weighted R2 score is: 0.013389268065182924\n",
      "date_id is: 74\n",
      "Test Weighted R2 score is: 0.01566503026681121\n",
      "date_id is: 75\n",
      "Test Weighted R2 score is: 0.00559387689095836\n",
      "date_id is: 76\n",
      "Test Weighted R2 score is: 0.01052287830417098\n",
      "date_id is: 77\n",
      "Test Weighted R2 score is: 0.033008515940229466\n",
      "date_id is: 78\n",
      "Test Weighted R2 score is: 0.0045083881475299625\n",
      "date_id is: 79\n",
      "Test Weighted R2 score is: 0.005749024700399841\n",
      "date_id is: 80\n",
      "Test Weighted R2 score is: 0.01150192224610913\n",
      "date_id is: 81\n",
      "Test Weighted R2 score is: -0.000510810150128993\n",
      "date_id is: 82\n",
      "Test Weighted R2 score is: 0.017779403086577217\n",
      "date_id is: 83\n",
      "Test Weighted R2 score is: 0.035742459015895456\n",
      "date_id is: 84\n",
      "Test Weighted R2 score is: 0.003182455553898733\n",
      "date_id is: 85\n",
      "Test Weighted R2 score is: 0.02438758247583339\n",
      "date_id is: 86\n",
      "Test Weighted R2 score is: 0.016329037161083915\n",
      "date_id is: 87\n",
      "Test Weighted R2 score is: -0.004477889513146094\n",
      "date_id is: 88\n",
      "Test Weighted R2 score is: 0.026734680113673415\n",
      "date_id is: 89\n",
      "Test Weighted R2 score is: 0.012380005684155093\n",
      "date_id is: 90\n",
      "Test Weighted R2 score is: 0.027671912709933744\n",
      "date_id is: 91\n",
      "Test Weighted R2 score is: 0.0304045311944503\n",
      "date_id is: 92\n",
      "Test Weighted R2 score is: 0.021639991014072102\n",
      "date_id is: 93\n",
      "Test Weighted R2 score is: 0.04525089768054247\n",
      "date_id is: 94\n",
      "Test Weighted R2 score is: 0.04368282553474023\n",
      "date_id is: 95\n",
      "Test Weighted R2 score is: 0.03075886420819418\n",
      "date_id is: 96\n",
      "Test Weighted R2 score is: 0.026725932263209407\n",
      "date_id is: 97\n",
      "Test Weighted R2 score is: 0.020707492787871096\n",
      "date_id is: 98\n",
      "Test Weighted R2 score is: 0.01537300118063345\n",
      "date_id is: 99\n",
      "Test Weighted R2 score is: 0.01231038085038505\n",
      "date_id is: 100\n",
      "Test Weighted R2 score is: 0.018048674003137588\n",
      "date_id is: 101\n",
      "Test Weighted R2 score is: 0.0315776584107863\n",
      "date_id is: 102\n",
      "Test Weighted R2 score is: 0.00834478342020084\n",
      "date_id is: 103\n",
      "Test Weighted R2 score is: 0.028476707805753843\n",
      "date_id is: 104\n",
      "Test Weighted R2 score is: 0.006426514313766951\n",
      "date_id is: 105\n",
      "Test Weighted R2 score is: 0.042685678315228626\n",
      "date_id is: 106\n",
      "Test Weighted R2 score is: 0.02104999056947776\n",
      "date_id is: 107\n",
      "Test Weighted R2 score is: 0.03413293903509351\n",
      "date_id is: 108\n",
      "Test Weighted R2 score is: 0.02286420600658423\n",
      "date_id is: 109\n",
      "Test Weighted R2 score is: 0.001309568177004783\n",
      "date_id is: 110\n",
      "Test Weighted R2 score is: 0.046859315442245864\n",
      "date_id is: 111\n",
      "Test Weighted R2 score is: 0.004982757405407101\n",
      "date_id is: 112\n",
      "Test Weighted R2 score is: 0.027422214790740584\n",
      "date_id is: 113\n",
      "Test Weighted R2 score is: 0.03302946103687043\n",
      "date_id is: 114\n",
      "Test Weighted R2 score is: 0.033402816246234535\n",
      "date_id is: 115\n",
      "Test Weighted R2 score is: 0.010374633505472297\n",
      "date_id is: 116\n",
      "Test Weighted R2 score is: 0.01132701759448751\n",
      "date_id is: 117\n",
      "Test Weighted R2 score is: 0.027327375893667183\n",
      "date_id is: 118\n",
      "Test Weighted R2 score is: 0.02054417323259261\n",
      "date_id is: 119\n",
      "Test Weighted R2 score is: 0.026000451419175175\n",
      "date_id is: 120\n",
      "Test Weighted R2 score is: 0.01807374493958125\n",
      "date_id is: 121\n",
      "Test Weighted R2 score is: 0.027983550002087187\n",
      "date_id is: 122\n",
      "Test Weighted R2 score is: 0.028577042368199024\n",
      "date_id is: 123\n",
      "Test Weighted R2 score is: 0.023955674322104215\n",
      "date_id is: 124\n",
      "Test Weighted R2 score is: 0.004422428963582359\n",
      "date_id is: 125\n",
      "Test Weighted R2 score is: 0.035510997163364366\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlgb_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, 300)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 108\u001b[0m, in \u001b[0;36mlgb_sliding_window\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m    105\u001b[0m weights_val \u001b[38;5;241m=\u001b[39m val_window_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m    106\u001b[0m weights_test \u001b[38;5;241m=\u001b[39m test_date_id_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m--> 108\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mweights_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, callbacks=[log_evaluation(period=10)])\u001b[39;00m\n\u001b[0;32m    110\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    112\u001b[0m test_score \u001b[38;5;241m=\u001b[39m r2_score(y_test, test_preds, sample_weight\u001b[38;5;241m=\u001b[39mweights_test)\n",
      "File \u001b[1;32mI:\\Kaggle\\kaggle_venvs\\ml\\Lib\\site-packages\\lightgbm\\sklearn.py:1189\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1174\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mI:\\Kaggle\\kaggle_venvs\\ml\\Lib\\site-packages\\lightgbm\\sklearn.py:955\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    952\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    953\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[1;32mI:\\Kaggle\\kaggle_venvs\\ml\\Lib\\site-packages\\lightgbm\\engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    296\u001b[0m     cb(\n\u001b[0;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[0;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    305\u001b[0m     )\n\u001b[1;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mI:\\Kaggle\\kaggle_venvs\\ml\\Lib\\site-packages\\lightgbm\\basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4135\u001b[0m _safe_call(\n\u001b[1;32m-> 4136\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4140\u001b[0m )\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgb_sliding_window(train_df)#, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c25d5-e8de-47d0-bc14-504cb01c1953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in lgb_study.best_params.keys():\n",
    "    fig = plot_slice(lgb_study, params=[param])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc374ce-73cb-4816-95c2-a48ad7549132",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(lgb_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d76fcf-0277-419f-9368-7c4c039ccc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219668b-f6cb-4a16-8260-6c95a3e8ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c749891-7ec2-4ea7-b4a3-40691e436f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in lgb_study.best_params.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752cca37-9145-43e5-aa81-2c40056df4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df = pd.DataFrame({k:[v] for k, v in lgb_study.best_params.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd04c5-e48c-429f-9095-d0bda03fafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753709a5-ac43-4152-9da4-6b7f9a7ec8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df.to_csv(models_path + 'lgb_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b262e51-9765-40a0-bec6-3418677f4f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
