{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6e3e10-78d2-413c-87c3-f256294da062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor, log_evaluation, record_evaluation\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "#from sklearn.impute import IterativeImputer\n",
    "import pickle\n",
    "import optuna\n",
    "from optuna.visualization import plot_slice, plot_param_importances\n",
    "import shap\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "pl.Config.set_tbl_rows(-1)\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_fmt_str_lengths(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8421cc58-8b61-4b48-8c20-3544d3a81f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5a256f-c55b-415d-8d12-e753283ffee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'I:/Kaggle/jane-street-real-time-market-data-forecasting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d215c3-e2a4-4ea2-8125-708737b10ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.csv',\n",
       " 'kaggle_evaluation',\n",
       " 'lags.parquet',\n",
       " 'my_folder',\n",
       " 'responders.csv',\n",
       " 'sample_submission.csv',\n",
       " 'team_folder',\n",
       " 'test.parquet',\n",
       " 'train.parquet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab47b6d-409b-41a7-a443-fcddc750f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47127338, 93)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 93)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_31</th><th>feature_32</th><th>feature_33</th><th>feature_34</th><th>feature_35</th><th>feature_36</th><th>feature_37</th><th>feature_38</th><th>feature_39</th><th>feature_40</th><th>feature_41</th><th>feature_42</th><th>feature_43</th><th>feature_44</th><th>feature_45</th><th>feature_46</th><th>feature_47</th><th>feature_48</th><th>feature_49</th><th>feature_50</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_6</th><th>responder_0_lag_1</th><th>responder_1_lag_1</th><th>responder_2_lag_1</th><th>responder_3_lag_1</th><th>responder_4_lag_1</th><th>responder_5_lag_1</th><th>responder_6_lag_1</th><th>responder_7_lag_1</th><th>responder_8_lag_1</th></tr><tr><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i8</td><td>i8</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>3.889038</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.851033</td><td>0.242971</td><td>0.2634</td><td>-0.891687</td><td>11</td><td>7</td><td>76</td><td>-0.883028</td><td>0.003067</td><td>-0.744703</td><td>null</td><td>-0.169586</td><td>null</td><td>-1.335938</td><td>-1.707803</td><td>0.91013</td><td>null</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>null</td><td>null</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>null</td><td>null</td><td>null</td><td>0.28118</td><td>0.269163</td><td>0.349028</td><td>-0.012596</td><td>-0.225932</td><td>null</td><td>-1.073602</td><td>null</td><td>null</td><td>-0.181716</td><td>null</td><td>null</td><td>null</td><td>0.564021</td><td>2.088506</td><td>0.832022</td><td>null</td><td>0.204797</td><td>null</td><td>null</td><td>-0.808103</td><td>null</td><td>-2.037683</td><td>0.727661</td><td>null</td><td>-0.989118</td><td>-0.345213</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.251104</td><td>-0.110252</td><td>-0.491157</td><td>-1.02269</td><td>0.152241</td><td>-0.659864</td><td>null</td><td>null</td><td>-0.261412</td><td>-0.211486</td><td>-0.335556</td><td>-0.281498</td><td>0.775981</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>7</td><td>1.370613</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.676961</td><td>0.151984</td><td>0.192465</td><td>-0.521729</td><td>11</td><td>7</td><td>76</td><td>-0.865307</td><td>-0.225629</td><td>-0.582163</td><td>null</td><td>0.317467</td><td>null</td><td>-1.250016</td><td>-1.682929</td><td>1.412757</td><td>null</td><td>0.520378</td><td>0.744132</td><td>-0.788658</td><td>0.641776</td><td>null</td><td>null</td><td>0.2272</td><td>0.580907</td><td>1.128879</td><td>null</td><td>null</td><td>null</td><td>-1.512286</td><td>-1.414357</td><td>-1.823322</td><td>-0.082763</td><td>-0.184119</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-10.835207</td><td>-0.002704</td><td>-0.621836</td><td>null</td><td>1.172836</td><td>null</td><td>null</td><td>-1.625862</td><td>null</td><td>-1.410017</td><td>1.063013</td><td>null</td><td>0.888355</td><td>0.467994</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.065759</td><td>0.013322</td><td>-0.592855</td><td>-1.052685</td><td>-0.393726</td><td>-0.741603</td><td>null</td><td>null</td><td>-0.281207</td><td>-0.182894</td><td>-0.245565</td><td>-0.302441</td><td>0.703665</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>9</td><td>2.285698</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.056285</td><td>0.187227</td><td>0.249901</td><td>-0.77305</td><td>11</td><td>7</td><td>76</td><td>-0.675719</td><td>-0.199404</td><td>-0.586798</td><td>null</td><td>-0.814909</td><td>null</td><td>-1.296782</td><td>-2.040234</td><td>0.639589</td><td>null</td><td>1.597359</td><td>0.657514</td><td>-1.350148</td><td>0.364215</td><td>null</td><td>null</td><td>-0.017751</td><td>-0.317361</td><td>-0.122379</td><td>null</td><td>null</td><td>null</td><td>-0.320921</td><td>-0.95809</td><td>-2.436589</td><td>0.070999</td><td>-0.245239</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.420632</td><td>-3.515137</td><td>-4.67776</td><td>null</td><td>0.535897</td><td>null</td><td>null</td><td>-0.72542</td><td>null</td><td>-2.29417</td><td>1.764551</td><td>null</td><td>-0.120789</td><td>-0.063458</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.882604</td><td>-0.072482</td><td>-0.617934</td><td>-0.86323</td><td>-0.241892</td><td>-0.709919</td><td>null</td><td>null</td><td>0.377131</td><td>0.300724</td><td>-0.106842</td><td>-0.096792</td><td>2.109352</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>10</td><td>0.690606</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.139366</td><td>0.273328</td><td>0.306549</td><td>-1.262223</td><td>42</td><td>5</td><td>150</td><td>-0.694008</td><td>3.004091</td><td>0.114809</td><td>null</td><td>-0.251882</td><td>null</td><td>-1.902009</td><td>-0.979447</td><td>0.241165</td><td>null</td><td>-0.392359</td><td>-0.224699</td><td>-2.129397</td><td>-0.855287</td><td>null</td><td>null</td><td>0.404142</td><td>-0.578156</td><td>0.105702</td><td>null</td><td>null</td><td>null</td><td>0.544138</td><td>-0.087091</td><td>-1.500147</td><td>-0.201288</td><td>-0.038042</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.382074</td><td>2.669135</td><td>0.611711</td><td>null</td><td>2.413415</td><td>null</td><td>null</td><td>1.313203</td><td>null</td><td>-0.810125</td><td>2.939022</td><td>null</td><td>3.988801</td><td>1.834661</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.697595</td><td>1.074309</td><td>-0.206929</td><td>-0.530602</td><td>4.765215</td><td>0.571554</td><td>null</td><td>null</td><td>-0.226891</td><td>-0.251412</td><td>-0.215522</td><td>-0.296244</td><td>1.114137</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>14</td><td>0.44057</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.9552</td><td>0.262404</td><td>0.344457</td><td>-0.613813</td><td>44</td><td>3</td><td>16</td><td>-0.947351</td><td>-0.030018</td><td>-0.502379</td><td>null</td><td>0.646086</td><td>null</td><td>-1.844685</td><td>-1.58656</td><td>-0.182024</td><td>null</td><td>-0.969949</td><td>-0.673813</td><td>-1.282132</td><td>-1.399894</td><td>null</td><td>null</td><td>0.043815</td><td>-0.320225</td><td>-0.031713</td><td>null</td><td>null</td><td>null</td><td>-0.08842</td><td>-0.995003</td><td>-2.635336</td><td>-0.196461</td><td>-0.618719</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-2.0146</td><td>-2.321076</td><td>-3.711265</td><td>null</td><td>1.253902</td><td>null</td><td>null</td><td>0.476195</td><td>null</td><td>-0.771732</td><td>2.843421</td><td>null</td><td>1.379815</td><td>0.411827</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.948601</td><td>-0.136814</td><td>-0.447704</td><td>-1.141761</td><td>0.099631</td><td>-0.661928</td><td>null</td><td>null</td><td>3.678076</td><td>2.793581</td><td>2.61825</td><td>3.418133</td><td>-3.57282</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 93)\n",
       "┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n",
       "│ dat ┆ tim ┆ sym ┆ wei ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ fea ┆ res ┆ res ┆ res ┆ res ┆ res ┆ res ┆ res ┆ res ┆ res ┆ res │\n",
       "│ e_i ┆ e_i ┆ bol ┆ ght ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ tur ┆ pon ┆ pon ┆ pon ┆ pon ┆ pon ┆ pon ┆ pon ┆ pon ┆ pon ┆ pon │\n",
       "│ d   ┆ d   ┆ _id ┆ --- ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_0 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_1 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_2 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_3 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_4 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_5 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_6 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ e_7 ┆ der ┆ der ┆ der ┆ der ┆ der ┆ der ┆ der ┆ der ┆ der ┆ der │\n",
       "│ --- ┆ --- ┆ --- ┆ f32 ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ 9   ┆ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   ┆ 6   ┆ 7   ┆ 8   ┆ _6  ┆ _0_ ┆ _1_ ┆ _2_ ┆ _3_ ┆ _4_ ┆ _5_ ┆ _6_ ┆ _7_ ┆ _8_ │\n",
       "│ i16 ┆ i16 ┆ i8  ┆     ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ lag ┆ lag ┆ lag ┆ lag ┆ lag ┆ lag ┆ lag ┆ lag ┆ lag │\n",
       "│     ┆     ┆     ┆     ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ i8  ┆ i8  ┆ i16 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ _1  ┆ _1  ┆ _1  ┆ _1  ┆ _1  ┆ _1  ┆ _1  ┆ _1  ┆ _1  │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 ┆ f32 │\n",
       "╞═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╡\n",
       "│ 0   ┆ 0   ┆ 1   ┆ 3.8 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.8 ┆ 0.2 ┆ 0.2 ┆ -0. ┆ 11  ┆ 7   ┆ 76  ┆ -0. ┆ 0.0 ┆ -0. ┆ nul ┆ -0. ┆ nul ┆ -1. ┆ -1. ┆ 0.9 ┆ nul ┆ 1.6 ┆ 1.5 ┆ -1. ┆ -0. ┆ nul ┆ nul ┆ 1.3 ┆ -0. ┆ 0.1 ┆ nul ┆ nul ┆ nul ┆ 0.2 ┆ 0.2 ┆ 0.3 ┆ -0. ┆ -0. ┆ nul ┆ -1. ┆ nul ┆ nul ┆ -0. ┆ nul ┆ nul ┆ nul ┆ 0.5 ┆ 2.0 ┆ 0.8 ┆ nul ┆ 0.2 ┆ nul ┆ nul ┆ -0. ┆ nul ┆ -2. ┆ 0.7 ┆ nul ┆ -0. ┆ -0. ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -1. ┆ -0. ┆ -0. ┆ -1. ┆ 0.1 ┆ -0. ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 0.7 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul │\n",
       "│     ┆     ┆     ┆ 890 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 510 ┆ 429 ┆ 634 ┆ 891 ┆     ┆     ┆     ┆ 883 ┆ 030 ┆ 744 ┆ l   ┆ 169 ┆ l   ┆ 335 ┆ 707 ┆ 101 ┆ l   ┆ 364 ┆ 221 ┆ 551 ┆ 229 ┆ l   ┆ l   ┆ 783 ┆ 283 ┆ 231 ┆ l   ┆ l   ┆ l   ┆ 811 ┆ 691 ┆ 490 ┆ 012 ┆ 225 ┆ l   ┆ 073 ┆ l   ┆ l   ┆ 181 ┆ l   ┆ l   ┆ l   ┆ 640 ┆ 885 ┆ 320 ┆ l   ┆ 047 ┆ l   ┆ l   ┆ 808 ┆ l   ┆ 037 ┆ 276 ┆ l   ┆ 989 ┆ 345 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 251 ┆ 110 ┆ 491 ┆ 022 ┆ 522 ┆ 659 ┆ l   ┆ l   ┆ 261 ┆ 211 ┆ 335 ┆ 281 ┆ 759 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   │\n",
       "│     ┆     ┆     ┆ 38  ┆     ┆     ┆     ┆     ┆     ┆ 33  ┆ 71  ┆     ┆ 687 ┆     ┆     ┆     ┆ 028 ┆ 67  ┆ 703 ┆     ┆ 586 ┆     ┆ 938 ┆ 803 ┆ 3   ┆     ┆ 31  ┆ 33  ┆ 398 ┆ 627 ┆     ┆     ┆ 01  ┆ 712 ┆ 96  ┆     ┆     ┆     ┆ 8   ┆ 63  ┆ 28  ┆ 596 ┆ 932 ┆     ┆ 602 ┆     ┆     ┆ 716 ┆     ┆     ┆     ┆ 21  ┆ 06  ┆ 22  ┆     ┆ 97  ┆     ┆     ┆ 103 ┆     ┆ 683 ┆ 61  ┆     ┆ 118 ┆ 213 ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 104 ┆ 252 ┆ 157 ┆ 69  ┆ 41  ┆ 864 ┆     ┆     ┆ 412 ┆ 486 ┆ 556 ┆ 498 ┆ 81  ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ 0   ┆ 0   ┆ 7   ┆ 1.3 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.6 ┆ 0.1 ┆ 0.1 ┆ -0. ┆ 11  ┆ 7   ┆ 76  ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ 0.3 ┆ nul ┆ -1. ┆ -1. ┆ 1.4 ┆ nul ┆ 0.5 ┆ 0.7 ┆ -0. ┆ 0.6 ┆ nul ┆ nul ┆ 0.2 ┆ 0.5 ┆ 1.1 ┆ nul ┆ nul ┆ nul ┆ -1. ┆ -1. ┆ -1. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -10 ┆ -0. ┆ -0. ┆ nul ┆ 1.1 ┆ nul ┆ nul ┆ -1. ┆ nul ┆ -1. ┆ 1.0 ┆ nul ┆ 0.8 ┆ 0.4 ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -1. ┆ 0.0 ┆ -0. ┆ -1. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 0.7 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul │\n",
       "│     ┆     ┆     ┆ 706 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 769 ┆ 519 ┆ 924 ┆ 521 ┆     ┆     ┆     ┆ 865 ┆ 225 ┆ 582 ┆ l   ┆ 174 ┆ l   ┆ 250 ┆ 682 ┆ 127 ┆ l   ┆ 203 ┆ 441 ┆ 788 ┆ 417 ┆ l   ┆ l   ┆ 272 ┆ 809 ┆ 288 ┆ l   ┆ l   ┆ l   ┆ 512 ┆ 414 ┆ 823 ┆ 082 ┆ 184 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ .83 ┆ 002 ┆ 621 ┆ l   ┆ 728 ┆ l   ┆ l   ┆ 625 ┆ l   ┆ 410 ┆ 630 ┆ l   ┆ 883 ┆ 679 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 065 ┆ 133 ┆ 592 ┆ 052 ┆ 393 ┆ 741 ┆ l   ┆ l   ┆ 281 ┆ 182 ┆ 245 ┆ 302 ┆ 036 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   │\n",
       "│     ┆     ┆     ┆ 13  ┆     ┆     ┆     ┆     ┆     ┆ 61  ┆ 84  ┆ 65  ┆ 729 ┆     ┆     ┆     ┆ 307 ┆ 629 ┆ 163 ┆     ┆ 67  ┆     ┆ 016 ┆ 929 ┆ 57  ┆     ┆ 78  ┆ 32  ┆ 658 ┆ 76  ┆     ┆     ┆     ┆ 07  ┆ 79  ┆     ┆     ┆     ┆ 286 ┆ 357 ┆ 322 ┆ 763 ┆ 119 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 520 ┆ 704 ┆ 836 ┆     ┆ 36  ┆     ┆     ┆ 862 ┆     ┆ 017 ┆ 13  ┆     ┆ 55  ┆ 94  ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 759 ┆ 22  ┆ 855 ┆ 685 ┆ 726 ┆ 603 ┆     ┆     ┆ 207 ┆ 894 ┆ 565 ┆ 441 ┆ 65  ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ 0   ┆ 0   ┆ 9   ┆ 2.2 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 1.0 ┆ 0.1 ┆ 0.2 ┆ -0. ┆ 11  ┆ 7   ┆ 76  ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ -0. ┆ nul ┆ -1. ┆ -2. ┆ 0.6 ┆ nul ┆ 1.5 ┆ 0.6 ┆ -1. ┆ 0.3 ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -2. ┆ 0.0 ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -1. ┆ -3. ┆ -4. ┆ nul ┆ 0.5 ┆ nul ┆ nul ┆ -0. ┆ nul ┆ -2. ┆ 1.7 ┆ nul ┆ -0. ┆ -0. ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ 0.3 ┆ 0.3 ┆ -0. ┆ -0. ┆ 2.1 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul │\n",
       "│     ┆     ┆     ┆ 856 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 562 ┆ 872 ┆ 499 ┆ 773 ┆     ┆     ┆     ┆ 675 ┆ 199 ┆ 586 ┆ l   ┆ 814 ┆ l   ┆ 296 ┆ 040 ┆ 395 ┆ l   ┆ 973 ┆ 575 ┆ 350 ┆ 642 ┆ l   ┆ l   ┆ 017 ┆ 317 ┆ 122 ┆ l   ┆ l   ┆ l   ┆ 320 ┆ 958 ┆ 436 ┆ 709 ┆ 245 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 420 ┆ 515 ┆ 677 ┆ l   ┆ 358 ┆ l   ┆ l   ┆ 725 ┆ l   ┆ 294 ┆ 645 ┆ l   ┆ 120 ┆ 063 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 882 ┆ 072 ┆ 617 ┆ 863 ┆ 241 ┆ 709 ┆ l   ┆ l   ┆ 771 ┆ 007 ┆ 106 ┆ 096 ┆ 093 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   │\n",
       "│     ┆     ┆     ┆ 98  ┆     ┆     ┆     ┆     ┆     ┆ 85  ┆ 27  ┆ 01  ┆ 05  ┆     ┆     ┆     ┆ 719 ┆ 404 ┆ 798 ┆     ┆ 909 ┆     ┆ 782 ┆ 234 ┆ 89  ┆     ┆ 59  ┆ 14  ┆ 148 ┆ 15  ┆     ┆     ┆ 751 ┆ 361 ┆ 379 ┆     ┆     ┆     ┆ 921 ┆ 09  ┆ 589 ┆ 99  ┆ 239 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 632 ┆ 137 ┆ 76  ┆     ┆ 97  ┆     ┆     ┆ 42  ┆     ┆ 17  ┆ 51  ┆     ┆ 789 ┆ 458 ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 604 ┆ 482 ┆ 934 ┆ 23  ┆ 892 ┆ 919 ┆     ┆     ┆ 31  ┆ 24  ┆ 842 ┆ 792 ┆ 52  ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ 0   ┆ 0   ┆ 10  ┆ 0.6 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 1.1 ┆ 0.2 ┆ 0.3 ┆ -1. ┆ 42  ┆ 5   ┆ 150 ┆ -0. ┆ 3.0 ┆ 0.1 ┆ nul ┆ -0. ┆ nul ┆ -1. ┆ -0. ┆ 0.2 ┆ nul ┆ -0. ┆ -0. ┆ -2. ┆ -0. ┆ nul ┆ nul ┆ 0.4 ┆ -0. ┆ 0.1 ┆ nul ┆ nul ┆ nul ┆ 0.5 ┆ -0. ┆ -1. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.3 ┆ 2.6 ┆ 0.6 ┆ nul ┆ 2.4 ┆ nul ┆ nul ┆ 1.3 ┆ nul ┆ -0. ┆ 2.9 ┆ nul ┆ 3.9 ┆ 1.8 ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -0. ┆ 1.0 ┆ -0. ┆ -0. ┆ 4.7 ┆ 0.5 ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -0. ┆ 1.1 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul │\n",
       "│     ┆     ┆     ┆ 906 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 393 ┆ 733 ┆ 065 ┆ 262 ┆     ┆     ┆     ┆ 694 ┆ 040 ┆ 148 ┆ l   ┆ 251 ┆ l   ┆ 902 ┆ 979 ┆ 411 ┆ l   ┆ 392 ┆ 224 ┆ 129 ┆ 855 ┆ l   ┆ l   ┆ 041 ┆ 578 ┆ 057 ┆ l   ┆ l   ┆ l   ┆ 441 ┆ 087 ┆ 500 ┆ 201 ┆ 038 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 820 ┆ 691 ┆ 117 ┆ l   ┆ 134 ┆ l   ┆ l   ┆ 132 ┆ l   ┆ 810 ┆ 390 ┆ l   ┆ 888 ┆ 346 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 697 ┆ 743 ┆ 206 ┆ 530 ┆ 652 ┆ 715 ┆ l   ┆ l   ┆ 226 ┆ 251 ┆ 215 ┆ 296 ┆ 141 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   │\n",
       "│     ┆     ┆     ┆ 06  ┆     ┆     ┆     ┆     ┆     ┆ 66  ┆ 28  ┆ 49  ┆ 223 ┆     ┆     ┆     ┆ 008 ┆ 91  ┆ 09  ┆     ┆ 882 ┆     ┆ 009 ┆ 447 ┆ 65  ┆     ┆ 359 ┆ 699 ┆ 397 ┆ 287 ┆     ┆     ┆ 42  ┆ 156 ┆ 02  ┆     ┆     ┆     ┆ 38  ┆ 091 ┆ 147 ┆ 288 ┆ 042 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 74  ┆ 35  ┆ 11  ┆     ┆ 15  ┆     ┆     ┆ 03  ┆     ┆ 125 ┆ 22  ┆     ┆ 01  ┆ 61  ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 595 ┆ 09  ┆ 929 ┆ 602 ┆ 15  ┆ 54  ┆     ┆     ┆ 891 ┆ 412 ┆ 522 ┆ 244 ┆ 37  ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "│ 0   ┆ 0   ┆ 14  ┆ 0.4 ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ 0.9 ┆ 0.2 ┆ 0.3 ┆ -0. ┆ 44  ┆ 3   ┆ 16  ┆ -0. ┆ -0. ┆ -0. ┆ nul ┆ 0.6 ┆ nul ┆ -1. ┆ -1. ┆ -0. ┆ nul ┆ -0. ┆ -0. ┆ -1. ┆ -1. ┆ nul ┆ nul ┆ 0.0 ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -2. ┆ -0. ┆ -0. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -2. ┆ -2. ┆ -3. ┆ nul ┆ 1.2 ┆ nul ┆ nul ┆ 0.4 ┆ nul ┆ -0. ┆ 2.8 ┆ nul ┆ 1.3 ┆ 0.4 ┆ -1. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ -0. ┆ -0. ┆ -0. ┆ -1. ┆ 0.0 ┆ -0. ┆ nul ┆ nul ┆ 3.6 ┆ 2.7 ┆ 2.6 ┆ 3.4 ┆ -3. ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul ┆ nul │\n",
       "│     ┆     ┆     ┆ 405 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 552 ┆ 624 ┆ 444 ┆ 613 ┆     ┆     ┆     ┆ 947 ┆ 030 ┆ 502 ┆ l   ┆ 460 ┆ l   ┆ 844 ┆ 586 ┆ 182 ┆ l   ┆ 969 ┆ 673 ┆ 282 ┆ 399 ┆ l   ┆ l   ┆ 438 ┆ 320 ┆ 031 ┆ l   ┆ l   ┆ l   ┆ 088 ┆ 995 ┆ 635 ┆ 196 ┆ 618 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 014 ┆ 321 ┆ 711 ┆ l   ┆ 539 ┆ l   ┆ l   ┆ 761 ┆ l   ┆ 771 ┆ 434 ┆ l   ┆ 798 ┆ 118 ┆ 362 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ 948 ┆ 136 ┆ 447 ┆ 141 ┆ 996 ┆ 661 ┆ l   ┆ l   ┆ 780 ┆ 935 ┆ 182 ┆ 181 ┆ 572 ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   ┆ l   │\n",
       "│     ┆     ┆     ┆ 7   ┆     ┆     ┆     ┆     ┆     ┆     ┆ 04  ┆ 57  ┆ 813 ┆     ┆     ┆     ┆ 351 ┆ 018 ┆ 379 ┆     ┆ 86  ┆     ┆ 685 ┆ 56  ┆ 024 ┆     ┆ 949 ┆ 813 ┆ 132 ┆ 894 ┆     ┆     ┆ 15  ┆ 225 ┆ 713 ┆     ┆     ┆     ┆ 42  ┆ 003 ┆ 336 ┆ 461 ┆ 719 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ 6   ┆ 076 ┆ 265 ┆     ┆ 02  ┆     ┆     ┆ 95  ┆     ┆ 732 ┆ 21  ┆     ┆ 15  ┆ 27  ┆ 24  ┆     ┆     ┆     ┆     ┆     ┆ 601 ┆ 814 ┆ 704 ┆ 761 ┆ 31  ┆ 928 ┆     ┆     ┆ 76  ┆ 81  ┆ 5   ┆ 33  ┆ 82  ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pl.read_parquet(path + 'train.parquet/').select(pl.all().shrink_dtype())\n",
    "lags_df = train_df.with_columns(pl.col('date_id') + 1).drop(['weight', 'partition_id'] + [col for col in train_df.columns if 'feature' in col]).rename({f'responder_{x}': f'responder_{x}_lag_1' for x in range(9)})\n",
    "train_df = train_df.drop(['responder_0', 'responder_1', 'responder_2', 'responder_3', 'responder_4', 'responder_5', 'responder_7', 'responder_8', 'partition_id']).select(pl.all().shrink_dtype())\n",
    "train_df = train_df.join(lags_df, on=['date_id', 'time_id', 'symbol_id'], how='left').select(pl.all().shrink_dtype())\n",
    "del lags_df\n",
    "gc.collect()\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7434dcc6-f9b9-4e8a-aa41-50c724fd1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scan = pl.scan_parquet(path + 'train.parquet/')\n",
    "test_scan = pl.scan_parquet(path + 'test.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93569971-385d-4e5a-b362-ff4fbf80cf6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_symbol_ids_list = sorted(train_scan.select('symbol_id').unique().collect()['symbol_id'].to_list())\n",
    "test_symbol_ids_list = sorted(test_scan.select('symbol_id').unique().collect()['symbol_id'].to_list())\n",
    "unique_symbol_ids_list = sorted(list(set(train_symbol_ids_list + test_symbol_ids_list)))\n",
    "unique_symbol_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d27779c-c935-48d8-a8ce-42a39e8e1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cat_cols(df):\n",
    "    for v in tqdm(unique_symbol_ids_list):\n",
    "        new_col_name = 'symbol_id_' + str(v)\n",
    "        #df[new_col_name] = (df['symbol_id'] == v).astype(int)\n",
    "        df = df.with_columns((pl.col('symbol_id') == v).cast(pl.Int8).alias(new_col_name))\n",
    "\n",
    "    \n",
    "    #df = df.drop('symbol_id', axis=1)\n",
    "\n",
    "    return df.select(pl.all().shrink_dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3834f8e8-0f10-4891-a325-5d288f99b503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.032444032"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.estimated_size() / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3ffe03-76cd-4ee5-bf7c-940de4646a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = path + 'my_folder/models/20250105_07/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e04716-a5a1-471c-b452-0a8b5f7a7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_online_learning(train_data):\n",
    "    weights = train_data['weight'].to_pandas()\n",
    "    y = train_data['responder_6'].to_pandas()\n",
    "\n",
    "    unique_date_ids = train_data['date_id'].unique()    \n",
    "    train_date_id_cut = 1300\n",
    "\n",
    "    print('max date:', unique_date_ids.max())\n",
    "    print('date id cut:', train_date_id_cut)\n",
    "\n",
    "    X_train = train_data.filter(pl.col('date_id') <= train_date_id_cut).drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()#.sample(frac=0.01)\n",
    "    X_val = train_data.filter(pl.col('date_id') > train_date_id_cut).drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "\n",
    "    print(X_train.shape[0] / train_data.shape[0])\n",
    "\n",
    "    y_train = y.loc[X_train.index]\n",
    "    y_val = y[-X_val.shape[0]:]\n",
    "\n",
    "    weights_train = weights.loc[X_train.index]\n",
    "    weights_val = weights[-X_val.shape[0]:]\n",
    "\n",
    "    print(X_train.shape)\n",
    "    display(X_train.head())\n",
    "    display(X_train.tail())\n",
    "    \n",
    "\n",
    "    #train_dataset = lgb.Dataset(data=X_train, label=y_train, weight=weights_train)\n",
    "    #val_dataset = lgb.Dataset(data=X_val, label=y_val, weight=weights_val)\n",
    "\n",
    "    base_params = {\n",
    "        'verbosity': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'device': 'gpu',\n",
    "        'early_stopping_round': 30,\n",
    "        'lambda_l2': 100,\n",
    "        #'metric': 'r2',\n",
    "        #'seed': 42\n",
    "    }\n",
    "\n",
    "    '''model = lgb.train(\n",
    "        params=base_params,\n",
    "        train_set=train_dataset,\n",
    "        num_boost_round=90\n",
    "    )'''\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        **base_params,\n",
    "        n_estimators=90000\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val], callbacks=[log_evaluation(period=10)])#, init_model=current_model)\n",
    "    #model.fit(X_train, y_train, sample_weight=weights_train)\n",
    "\n",
    "    best_iteration = model.best_iteration_\n",
    "    print(f\"Best iteration: {best_iteration}\")\n",
    "\n",
    "    plt.figure()\n",
    "    lgb.plot_metric(model)\n",
    "    plt.ylim(0, 2)\n",
    "    plt.show()\n",
    "\n",
    "    val_preds = model.predict(X_val)\n",
    "\n",
    "    print('Val Weighted R2 score is:', r2_score(y_val, val_preds, sample_weight=weights_val))\n",
    "\n",
    "    return model\n",
    "\n",
    "    val_date_ids = sorted(train_data.filter(pl.col('date_id') > train_date_id_cut)['date_id'].unique())\n",
    "    \n",
    "    for date_id_v in val_date_ids:\n",
    "        for time_id_v in sorted(train_data.filter(pl.col('date_id') == date_id_v)['time_id'].unique()):\n",
    "            time_id_df = train_data.filter((pl.col('date_id') == date_id_v) & (pl.col('time_id') == time_id_v))\n",
    "\n",
    "            print(time_id_df.shape)\n",
    "            display(time_id_df)\n",
    "\n",
    "            time_id_X_train = time_id_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "            time_id_y_train = time_id_df['responder_6'].to_pandas()\n",
    "            time_id_weights_train = time_id_df['weight'].to_pandas()\n",
    "\n",
    "            val_data_df = train_data.filter(pl.col('date_id') >= date_id_v)[time_id_df.shape[0]:]\n",
    "\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return\n",
    "    \n",
    "    '''weights = train_data['weight']\n",
    "    y = train_data['responder_6']\n",
    "    \n",
    "    unique_date_ids = train_data['date_id'].unique()\n",
    "    train_date_id_cut = int(unique_date_ids.max() - 10)\n",
    "\n",
    "    print('max date:', unique_date_ids.max())\n",
    "    print('date id cut:', train_date_id_cut)\n",
    "    \n",
    "    X_train = train_data.filter(pl.col('date_id') <= train_date_id_cut).drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "    X_val = train_data.filter(pl.col('date_id') > train_date_id_cut).drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "\n",
    "    print(X_train.shape[0] / train_data.shape[0])\n",
    "    \n",
    "    y_train = y[:X_train.shape[0]].to_pandas()\n",
    "    y_val = y[X_train.shape[0]:].to_pandas()\n",
    "    \n",
    "    weights_train = weights[:X_train.shape[0]].to_pandas()\n",
    "    weights_val = weights[X_train.shape[0]:].to_pandas()\n",
    "\n",
    "    print(X_train.shape)\n",
    "    display(X_train.head())\n",
    "\n",
    "    base_params = {\n",
    "        'verbosity': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'device': 'gpu',\n",
    "        'early_stopping_round': 30,\n",
    "        'lambda_l2': 100\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(\n",
    "        **base_params,\n",
    "        n_estimators=100000\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val], callbacks=[log_evaluation(period=50)])#, categorical_feature=['symbol_id'])\n",
    "\n",
    "    best_iteration = model.best_iteration_\n",
    "    print(f\"Best iteration: {best_iteration}\")\n",
    "\n",
    "    val_preds = model.predict(X_val)\n",
    "\n",
    "    plt.figure()\n",
    "    lgb.plot_metric(model)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()    \n",
    "\n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "\n",
    "    with open(models_path + \"lgb_model.pkl\", 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print('Val Weighted R2 score is:', r2_score(y_val, val_preds, sample_weight=weights_val))\n",
    "\n",
    "    sample_val = X_val.sample(frac=0.001)\n",
    "    sample_y = y_val.loc[sample_val.index]\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X=sample_val, y=sample_y)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    del X_train, y_train, X_val, y_val, weights_train, weights_val\n",
    "    gc.collect()\n",
    "\n",
    "    # Retraining on the full dataset using best_iteration\n",
    "    X_full = train_data.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "    y_full = y.to_pandas()\n",
    "    weights_full = weights.to_pandas()\n",
    "\n",
    "    base_params.pop('early_stopping_round')\n",
    "\n",
    "    model_full = LGBMRegressor(\n",
    "        **base_params,\n",
    "        n_estimators=best_iteration\n",
    "    )\n",
    "    \n",
    "    model_full.fit(X_full, y_full, sample_weight=weights_full)\n",
    "\n",
    "    with open(models_path + \"lgb_model_full.pkl\", 'wb') as file:\n",
    "        pickle.dump(model_full, file)\n",
    "\n",
    "    print(\"Retraining complete. Model saved as 'lgb_model_full.pkl'.\")\n",
    "\n",
    "    return shap_importance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f11640b-066f-4b04-9b5d-344b1c861b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max date: 1698\n",
      "date id cut: 1300\n",
      "0.6895364639521969\n",
      "(32496018, 88)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>feature_08</th>\n",
       "      <th>feature_09</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851033</td>\n",
       "      <td>0.242971</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>-0.891687</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>-0.883028</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>-0.744703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.169586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.335938</td>\n",
       "      <td>-1.707803</td>\n",
       "      <td>0.910130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.636431</td>\n",
       "      <td>1.522133</td>\n",
       "      <td>-1.551398</td>\n",
       "      <td>-0.229627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.378301</td>\n",
       "      <td>-0.283712</td>\n",
       "      <td>0.123196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.281180</td>\n",
       "      <td>0.269163</td>\n",
       "      <td>0.349028</td>\n",
       "      <td>-0.012596</td>\n",
       "      <td>-0.225932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.073602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.181716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.564021</td>\n",
       "      <td>2.088506</td>\n",
       "      <td>0.832022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.204797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.808103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.037683</td>\n",
       "      <td>0.727661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.989118</td>\n",
       "      <td>-0.345213</td>\n",
       "      <td>-1.36224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.251104</td>\n",
       "      <td>-0.110252</td>\n",
       "      <td>-0.491157</td>\n",
       "      <td>-1.022690</td>\n",
       "      <td>0.152241</td>\n",
       "      <td>-0.659864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.261412</td>\n",
       "      <td>-0.211486</td>\n",
       "      <td>-0.335556</td>\n",
       "      <td>-0.281498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.676961</td>\n",
       "      <td>0.151984</td>\n",
       "      <td>0.192465</td>\n",
       "      <td>-0.521729</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>-0.865307</td>\n",
       "      <td>-0.225629</td>\n",
       "      <td>-0.582163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.317467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.250016</td>\n",
       "      <td>-1.682929</td>\n",
       "      <td>1.412757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520378</td>\n",
       "      <td>0.744132</td>\n",
       "      <td>-0.788658</td>\n",
       "      <td>0.641776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.580907</td>\n",
       "      <td>1.128879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.512286</td>\n",
       "      <td>-1.414357</td>\n",
       "      <td>-1.823322</td>\n",
       "      <td>-0.082763</td>\n",
       "      <td>-0.184119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.835207</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>-0.621836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.172836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.625862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.410017</td>\n",
       "      <td>1.063013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.888355</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>-1.36224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.065759</td>\n",
       "      <td>0.013322</td>\n",
       "      <td>-0.592855</td>\n",
       "      <td>-1.052685</td>\n",
       "      <td>-0.393726</td>\n",
       "      <td>-0.741603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.281207</td>\n",
       "      <td>-0.182894</td>\n",
       "      <td>-0.245565</td>\n",
       "      <td>-0.302441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.056285</td>\n",
       "      <td>0.187227</td>\n",
       "      <td>0.249901</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>-0.675719</td>\n",
       "      <td>-0.199404</td>\n",
       "      <td>-0.586798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.814909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.296782</td>\n",
       "      <td>-2.040234</td>\n",
       "      <td>0.639589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.597359</td>\n",
       "      <td>0.657514</td>\n",
       "      <td>-1.350148</td>\n",
       "      <td>0.364215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017751</td>\n",
       "      <td>-0.317361</td>\n",
       "      <td>-0.122379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.320921</td>\n",
       "      <td>-0.958090</td>\n",
       "      <td>-2.436589</td>\n",
       "      <td>0.070999</td>\n",
       "      <td>-0.245239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.420632</td>\n",
       "      <td>-3.515137</td>\n",
       "      <td>-4.677760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.725420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.294170</td>\n",
       "      <td>1.764551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.120789</td>\n",
       "      <td>-0.063458</td>\n",
       "      <td>-1.36224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.882604</td>\n",
       "      <td>-0.072482</td>\n",
       "      <td>-0.617934</td>\n",
       "      <td>-0.863230</td>\n",
       "      <td>-0.241892</td>\n",
       "      <td>-0.709919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.377131</td>\n",
       "      <td>0.300724</td>\n",
       "      <td>-0.106842</td>\n",
       "      <td>-0.096792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.139366</td>\n",
       "      <td>0.273328</td>\n",
       "      <td>0.306549</td>\n",
       "      <td>-1.262223</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>-0.694008</td>\n",
       "      <td>3.004091</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.251882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.902009</td>\n",
       "      <td>-0.979447</td>\n",
       "      <td>0.241165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.392359</td>\n",
       "      <td>-0.224699</td>\n",
       "      <td>-2.129397</td>\n",
       "      <td>-0.855287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404142</td>\n",
       "      <td>-0.578156</td>\n",
       "      <td>0.105702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.544138</td>\n",
       "      <td>-0.087091</td>\n",
       "      <td>-1.500147</td>\n",
       "      <td>-0.201288</td>\n",
       "      <td>-0.038042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382074</td>\n",
       "      <td>2.669135</td>\n",
       "      <td>0.611711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.413415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.313203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.810125</td>\n",
       "      <td>2.939022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.988801</td>\n",
       "      <td>1.834661</td>\n",
       "      <td>-1.36224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.697595</td>\n",
       "      <td>1.074309</td>\n",
       "      <td>-0.206929</td>\n",
       "      <td>-0.530602</td>\n",
       "      <td>4.765215</td>\n",
       "      <td>0.571554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.226891</td>\n",
       "      <td>-0.251412</td>\n",
       "      <td>-0.215522</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.262404</td>\n",
       "      <td>0.344457</td>\n",
       "      <td>-0.613813</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.947351</td>\n",
       "      <td>-0.030018</td>\n",
       "      <td>-0.502379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.646086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.844685</td>\n",
       "      <td>-1.586560</td>\n",
       "      <td>-0.182024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.969949</td>\n",
       "      <td>-0.673813</td>\n",
       "      <td>-1.282132</td>\n",
       "      <td>-1.399894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043815</td>\n",
       "      <td>-0.320225</td>\n",
       "      <td>-0.031713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.088420</td>\n",
       "      <td>-0.995003</td>\n",
       "      <td>-2.635336</td>\n",
       "      <td>-0.196461</td>\n",
       "      <td>-0.618719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.014600</td>\n",
       "      <td>-2.321076</td>\n",
       "      <td>-3.711265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.253902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.771732</td>\n",
       "      <td>2.843421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.379815</td>\n",
       "      <td>0.411827</td>\n",
       "      <td>-1.36224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.948601</td>\n",
       "      <td>-0.136814</td>\n",
       "      <td>-0.447704</td>\n",
       "      <td>-1.141761</td>\n",
       "      <td>0.099631</td>\n",
       "      <td>-0.661928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.678076</td>\n",
       "      <td>2.793581</td>\n",
       "      <td>2.618250</td>\n",
       "      <td>3.418133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_00  feature_01  feature_02  feature_03  feature_04  feature_05  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN    0.851033   \n",
       "1         NaN         NaN         NaN         NaN         NaN    0.676961   \n",
       "2         NaN         NaN         NaN         NaN         NaN    1.056285   \n",
       "3         NaN         NaN         NaN         NaN         NaN    1.139366   \n",
       "4         NaN         NaN         NaN         NaN         NaN    0.955200   \n",
       "\n",
       "   feature_06  feature_07  feature_08  feature_09  feature_10  feature_11  \\\n",
       "0    0.242971    0.263400   -0.891687          11           7          76   \n",
       "1    0.151984    0.192465   -0.521729          11           7          76   \n",
       "2    0.187227    0.249901   -0.773050          11           7          76   \n",
       "3    0.273328    0.306549   -1.262223          42           5         150   \n",
       "4    0.262404    0.344457   -0.613813          44           3          16   \n",
       "\n",
       "   feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \\\n",
       "0   -0.883028    0.003067   -0.744703         NaN   -0.169586         NaN   \n",
       "1   -0.865307   -0.225629   -0.582163         NaN    0.317467         NaN   \n",
       "2   -0.675719   -0.199404   -0.586798         NaN   -0.814909         NaN   \n",
       "3   -0.694008    3.004091    0.114809         NaN   -0.251882         NaN   \n",
       "4   -0.947351   -0.030018   -0.502379         NaN    0.646086         NaN   \n",
       "\n",
       "   feature_18  feature_19  feature_20  feature_21  feature_22  feature_23  \\\n",
       "0   -1.335938   -1.707803    0.910130         NaN    1.636431    1.522133   \n",
       "1   -1.250016   -1.682929    1.412757         NaN    0.520378    0.744132   \n",
       "2   -1.296782   -2.040234    0.639589         NaN    1.597359    0.657514   \n",
       "3   -1.902009   -0.979447    0.241165         NaN   -0.392359   -0.224699   \n",
       "4   -1.844685   -1.586560   -0.182024         NaN   -0.969949   -0.673813   \n",
       "\n",
       "   feature_24  feature_25  feature_26  feature_27  feature_28  feature_29  \\\n",
       "0   -1.551398   -0.229627         NaN         NaN    1.378301   -0.283712   \n",
       "1   -0.788658    0.641776         NaN         NaN    0.227200    0.580907   \n",
       "2   -1.350148    0.364215         NaN         NaN   -0.017751   -0.317361   \n",
       "3   -2.129397   -0.855287         NaN         NaN    0.404142   -0.578156   \n",
       "4   -1.282132   -1.399894         NaN         NaN    0.043815   -0.320225   \n",
       "\n",
       "   feature_30  feature_31  feature_32  feature_33  feature_34  feature_35  \\\n",
       "0    0.123196         NaN         NaN         NaN    0.281180    0.269163   \n",
       "1    1.128879         NaN         NaN         NaN   -1.512286   -1.414357   \n",
       "2   -0.122379         NaN         NaN         NaN   -0.320921   -0.958090   \n",
       "3    0.105702         NaN         NaN         NaN    0.544138   -0.087091   \n",
       "4   -0.031713         NaN         NaN         NaN   -0.088420   -0.995003   \n",
       "\n",
       "   feature_36  feature_37  feature_38  feature_39  feature_40  feature_41  \\\n",
       "0    0.349028   -0.012596   -0.225932         NaN   -1.073602         NaN   \n",
       "1   -1.823322   -0.082763   -0.184119         NaN         NaN         NaN   \n",
       "2   -2.436589    0.070999   -0.245239         NaN         NaN         NaN   \n",
       "3   -1.500147   -0.201288   -0.038042         NaN         NaN         NaN   \n",
       "4   -2.635336   -0.196461   -0.618719         NaN         NaN         NaN   \n",
       "\n",
       "   feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
       "0         NaN   -0.181716         NaN         NaN         NaN    0.564021   \n",
       "1         NaN         NaN         NaN         NaN         NaN  -10.835207   \n",
       "2         NaN         NaN         NaN         NaN         NaN   -1.420632   \n",
       "3         NaN         NaN         NaN         NaN         NaN    0.382074   \n",
       "4         NaN         NaN         NaN         NaN         NaN   -2.014600   \n",
       "\n",
       "   feature_48  feature_49  feature_50  feature_51  feature_52  feature_53  \\\n",
       "0    2.088506    0.832022         NaN    0.204797         NaN         NaN   \n",
       "1   -0.002704   -0.621836         NaN    1.172836         NaN         NaN   \n",
       "2   -3.515137   -4.677760         NaN    0.535897         NaN         NaN   \n",
       "3    2.669135    0.611711         NaN    2.413415         NaN         NaN   \n",
       "4   -2.321076   -3.711265         NaN    1.253902         NaN         NaN   \n",
       "\n",
       "   feature_54  feature_55  feature_56  feature_57  feature_58  feature_59  \\\n",
       "0   -0.808103         NaN   -2.037683    0.727661         NaN   -0.989118   \n",
       "1   -1.625862         NaN   -1.410017    1.063013         NaN    0.888355   \n",
       "2   -0.725420         NaN   -2.294170    1.764551         NaN   -0.120789   \n",
       "3    1.313203         NaN   -0.810125    2.939022         NaN    3.988801   \n",
       "4    0.476195         NaN   -0.771732    2.843421         NaN    1.379815   \n",
       "\n",
       "   feature_60  feature_61  feature_62  feature_63  feature_64  feature_65  \\\n",
       "0   -0.345213    -1.36224         NaN         NaN         NaN         NaN   \n",
       "1    0.467994    -1.36224         NaN         NaN         NaN         NaN   \n",
       "2   -0.063458    -1.36224         NaN         NaN         NaN         NaN   \n",
       "3    1.834661    -1.36224         NaN         NaN         NaN         NaN   \n",
       "4    0.411827    -1.36224         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   feature_66  feature_67  feature_68  feature_69  feature_70  feature_71  \\\n",
       "0         NaN   -1.251104   -0.110252   -0.491157   -1.022690    0.152241   \n",
       "1         NaN   -1.065759    0.013322   -0.592855   -1.052685   -0.393726   \n",
       "2         NaN   -0.882604   -0.072482   -0.617934   -0.863230   -0.241892   \n",
       "3         NaN   -0.697595    1.074309   -0.206929   -0.530602    4.765215   \n",
       "4         NaN   -0.948601   -0.136814   -0.447704   -1.141761    0.099631   \n",
       "\n",
       "   feature_72  feature_73  feature_74  feature_75  feature_76  feature_77  \\\n",
       "0   -0.659864         NaN         NaN   -0.261412   -0.211486   -0.335556   \n",
       "1   -0.741603         NaN         NaN   -0.281207   -0.182894   -0.245565   \n",
       "2   -0.709919         NaN         NaN    0.377131    0.300724   -0.106842   \n",
       "3    0.571554         NaN         NaN   -0.226891   -0.251412   -0.215522   \n",
       "4   -0.661928         NaN         NaN    3.678076    2.793581    2.618250   \n",
       "\n",
       "   feature_78  responder_0_lag_1  responder_1_lag_1  responder_2_lag_1  \\\n",
       "0   -0.281498                NaN                NaN                NaN   \n",
       "1   -0.302441                NaN                NaN                NaN   \n",
       "2   -0.096792                NaN                NaN                NaN   \n",
       "3   -0.296244                NaN                NaN                NaN   \n",
       "4    3.418133                NaN                NaN                NaN   \n",
       "\n",
       "   responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  responder_6_lag_1  \\\n",
       "0                NaN                NaN                NaN                NaN   \n",
       "1                NaN                NaN                NaN                NaN   \n",
       "2                NaN                NaN                NaN                NaN   \n",
       "3                NaN                NaN                NaN                NaN   \n",
       "4                NaN                NaN                NaN                NaN   \n",
       "\n",
       "   responder_7_lag_1  responder_8_lag_1  \n",
       "0                NaN                NaN  \n",
       "1                NaN                NaN  \n",
       "2                NaN                NaN  \n",
       "3                NaN                NaN  \n",
       "4                NaN                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>feature_08</th>\n",
       "      <th>feature_09</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32496013</th>\n",
       "      <td>-0.273723</td>\n",
       "      <td>-0.628400</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.326928</td>\n",
       "      <td>0.120378</td>\n",
       "      <td>-0.017101</td>\n",
       "      <td>0.293846</td>\n",
       "      <td>0.267356</td>\n",
       "      <td>0.739021</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>0.886931</td>\n",
       "      <td>0.800840</td>\n",
       "      <td>0.619701</td>\n",
       "      <td>-0.944317</td>\n",
       "      <td>-0.731467</td>\n",
       "      <td>-0.826058</td>\n",
       "      <td>1.270973</td>\n",
       "      <td>2.309707</td>\n",
       "      <td>-0.168244</td>\n",
       "      <td>-0.180836</td>\n",
       "      <td>-0.228961</td>\n",
       "      <td>-0.731446</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>-0.058762</td>\n",
       "      <td>-1.233727</td>\n",
       "      <td>0.421452</td>\n",
       "      <td>1.658583</td>\n",
       "      <td>-0.643178</td>\n",
       "      <td>-0.641713</td>\n",
       "      <td>-0.127338</td>\n",
       "      <td>1.012934</td>\n",
       "      <td>-0.808432</td>\n",
       "      <td>0.640629</td>\n",
       "      <td>0.046857</td>\n",
       "      <td>-0.553982</td>\n",
       "      <td>-0.097041</td>\n",
       "      <td>-0.199727</td>\n",
       "      <td>-0.778504</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.259686</td>\n",
       "      <td>-0.897121</td>\n",
       "      <td>-0.245391</td>\n",
       "      <td>-0.181529</td>\n",
       "      <td>-0.041429</td>\n",
       "      <td>-0.114340</td>\n",
       "      <td>0.074312</td>\n",
       "      <td>-0.162039</td>\n",
       "      <td>-0.156312</td>\n",
       "      <td>-0.445739</td>\n",
       "      <td>-0.194293</td>\n",
       "      <td>-0.992565</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.776747</td>\n",
       "      <td>0.515383</td>\n",
       "      <td>-0.310368</td>\n",
       "      <td>0.715174</td>\n",
       "      <td>0.382011</td>\n",
       "      <td>-0.057844</td>\n",
       "      <td>-0.040984</td>\n",
       "      <td>0.796951</td>\n",
       "      <td>-0.526368</td>\n",
       "      <td>-0.235048</td>\n",
       "      <td>-0.354705</td>\n",
       "      <td>1.505933</td>\n",
       "      <td>1.735560</td>\n",
       "      <td>0.519725</td>\n",
       "      <td>0.034015</td>\n",
       "      <td>0.247436</td>\n",
       "      <td>1.266712</td>\n",
       "      <td>1.023329</td>\n",
       "      <td>1.049752</td>\n",
       "      <td>-0.020475</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>-0.234894</td>\n",
       "      <td>-0.328479</td>\n",
       "      <td>-0.167722</td>\n",
       "      <td>-0.107061</td>\n",
       "      <td>0.013119</td>\n",
       "      <td>0.299852</td>\n",
       "      <td>0.113701</td>\n",
       "      <td>0.191003</td>\n",
       "      <td>0.090730</td>\n",
       "      <td>-0.233323</td>\n",
       "      <td>-0.171773</td>\n",
       "      <td>-0.042368</td>\n",
       "      <td>-0.313162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32496014</th>\n",
       "      <td>0.516346</td>\n",
       "      <td>-0.358216</td>\n",
       "      <td>0.170515</td>\n",
       "      <td>0.457404</td>\n",
       "      <td>0.704279</td>\n",
       "      <td>-0.016830</td>\n",
       "      <td>0.200757</td>\n",
       "      <td>0.196041</td>\n",
       "      <td>1.106106</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>0.569555</td>\n",
       "      <td>0.320779</td>\n",
       "      <td>0.445927</td>\n",
       "      <td>0.833764</td>\n",
       "      <td>0.766083</td>\n",
       "      <td>0.760303</td>\n",
       "      <td>2.978000</td>\n",
       "      <td>0.180232</td>\n",
       "      <td>-0.071296</td>\n",
       "      <td>0.833885</td>\n",
       "      <td>-0.442648</td>\n",
       "      <td>1.909253</td>\n",
       "      <td>1.260226</td>\n",
       "      <td>2.173384</td>\n",
       "      <td>0.124759</td>\n",
       "      <td>-1.742508</td>\n",
       "      <td>-2.060564</td>\n",
       "      <td>2.431904</td>\n",
       "      <td>0.894903</td>\n",
       "      <td>1.336326</td>\n",
       "      <td>0.233143</td>\n",
       "      <td>0.268164</td>\n",
       "      <td>0.180035</td>\n",
       "      <td>-0.047150</td>\n",
       "      <td>-0.035961</td>\n",
       "      <td>1.073607</td>\n",
       "      <td>0.868699</td>\n",
       "      <td>1.179430</td>\n",
       "      <td>0.910744</td>\n",
       "      <td>0.365738</td>\n",
       "      <td>-0.806121</td>\n",
       "      <td>-1.221758</td>\n",
       "      <td>-0.294194</td>\n",
       "      <td>-0.309661</td>\n",
       "      <td>-1.669550</td>\n",
       "      <td>-1.185478</td>\n",
       "      <td>-0.013506</td>\n",
       "      <td>-0.296199</td>\n",
       "      <td>0.629851</td>\n",
       "      <td>1.619304</td>\n",
       "      <td>0.130224</td>\n",
       "      <td>-1.331440</td>\n",
       "      <td>-1.206886</td>\n",
       "      <td>-0.506745</td>\n",
       "      <td>-0.665731</td>\n",
       "      <td>-1.265526</td>\n",
       "      <td>-1.339285</td>\n",
       "      <td>0.179611</td>\n",
       "      <td>-0.155559</td>\n",
       "      <td>0.796951</td>\n",
       "      <td>-0.099937</td>\n",
       "      <td>0.127875</td>\n",
       "      <td>-0.027559</td>\n",
       "      <td>2.582265</td>\n",
       "      <td>-0.386582</td>\n",
       "      <td>1.081167</td>\n",
       "      <td>-0.309833</td>\n",
       "      <td>0.429599</td>\n",
       "      <td>0.702698</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>0.391108</td>\n",
       "      <td>-0.312883</td>\n",
       "      <td>-0.507126</td>\n",
       "      <td>-0.303390</td>\n",
       "      <td>-0.442164</td>\n",
       "      <td>-0.255057</td>\n",
       "      <td>-0.366450</td>\n",
       "      <td>-0.150453</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>-2.491776</td>\n",
       "      <td>0.146255</td>\n",
       "      <td>0.055375</td>\n",
       "      <td>-0.119033</td>\n",
       "      <td>0.025655</td>\n",
       "      <td>0.050160</td>\n",
       "      <td>0.044680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32496015</th>\n",
       "      <td>-0.276064</td>\n",
       "      <td>-0.562562</td>\n",
       "      <td>0.126326</td>\n",
       "      <td>0.341234</td>\n",
       "      <td>0.648612</td>\n",
       "      <td>-0.010778</td>\n",
       "      <td>0.217542</td>\n",
       "      <td>0.150108</td>\n",
       "      <td>0.763144</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>388</td>\n",
       "      <td>-0.154750</td>\n",
       "      <td>0.241596</td>\n",
       "      <td>-0.084922</td>\n",
       "      <td>-0.210215</td>\n",
       "      <td>-0.473479</td>\n",
       "      <td>-0.281191</td>\n",
       "      <td>0.932661</td>\n",
       "      <td>-1.085921</td>\n",
       "      <td>-0.415765</td>\n",
       "      <td>0.663795</td>\n",
       "      <td>-0.465737</td>\n",
       "      <td>1.354075</td>\n",
       "      <td>2.110133</td>\n",
       "      <td>2.677571</td>\n",
       "      <td>-0.370774</td>\n",
       "      <td>-2.367140</td>\n",
       "      <td>-1.442387</td>\n",
       "      <td>0.450709</td>\n",
       "      <td>-0.131176</td>\n",
       "      <td>0.483004</td>\n",
       "      <td>0.787535</td>\n",
       "      <td>-0.551896</td>\n",
       "      <td>0.947080</td>\n",
       "      <td>0.807590</td>\n",
       "      <td>-0.549592</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>0.180247</td>\n",
       "      <td>1.340250</td>\n",
       "      <td>-0.481007</td>\n",
       "      <td>1.146693</td>\n",
       "      <td>-0.012450</td>\n",
       "      <td>0.033310</td>\n",
       "      <td>-0.014503</td>\n",
       "      <td>0.447920</td>\n",
       "      <td>0.086581</td>\n",
       "      <td>-0.195873</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.530240</td>\n",
       "      <td>-0.618310</td>\n",
       "      <td>-0.746920</td>\n",
       "      <td>0.534145</td>\n",
       "      <td>-0.556152</td>\n",
       "      <td>0.391937</td>\n",
       "      <td>0.606587</td>\n",
       "      <td>1.020363</td>\n",
       "      <td>0.241115</td>\n",
       "      <td>-0.063700</td>\n",
       "      <td>0.080730</td>\n",
       "      <td>0.796951</td>\n",
       "      <td>-0.368261</td>\n",
       "      <td>-0.287799</td>\n",
       "      <td>-0.280969</td>\n",
       "      <td>0.449343</td>\n",
       "      <td>1.240352</td>\n",
       "      <td>-0.351033</td>\n",
       "      <td>0.879358</td>\n",
       "      <td>-0.191713</td>\n",
       "      <td>-0.081830</td>\n",
       "      <td>-0.143664</td>\n",
       "      <td>0.025583</td>\n",
       "      <td>-0.265242</td>\n",
       "      <td>-0.402117</td>\n",
       "      <td>-0.290549</td>\n",
       "      <td>-0.336317</td>\n",
       "      <td>-0.207405</td>\n",
       "      <td>-0.323209</td>\n",
       "      <td>-0.063781</td>\n",
       "      <td>0.286257</td>\n",
       "      <td>1.291592</td>\n",
       "      <td>0.117581</td>\n",
       "      <td>0.050176</td>\n",
       "      <td>-0.013990</td>\n",
       "      <td>-0.005065</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>-0.019257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32496016</th>\n",
       "      <td>0.048198</td>\n",
       "      <td>-0.705530</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>0.377266</td>\n",
       "      <td>-0.100474</td>\n",
       "      <td>-0.014540</td>\n",
       "      <td>0.407335</td>\n",
       "      <td>0.410498</td>\n",
       "      <td>0.778039</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>214</td>\n",
       "      <td>0.412368</td>\n",
       "      <td>1.015218</td>\n",
       "      <td>0.308765</td>\n",
       "      <td>-0.950215</td>\n",
       "      <td>-0.248845</td>\n",
       "      <td>-0.995436</td>\n",
       "      <td>0.486453</td>\n",
       "      <td>1.312123</td>\n",
       "      <td>-1.498882</td>\n",
       "      <td>-0.192676</td>\n",
       "      <td>-0.928796</td>\n",
       "      <td>-1.160311</td>\n",
       "      <td>0.198818</td>\n",
       "      <td>-0.378694</td>\n",
       "      <td>0.325012</td>\n",
       "      <td>-1.749679</td>\n",
       "      <td>-1.901364</td>\n",
       "      <td>-0.891058</td>\n",
       "      <td>-0.919408</td>\n",
       "      <td>-0.148865</td>\n",
       "      <td>-0.459053</td>\n",
       "      <td>-0.338284</td>\n",
       "      <td>0.518999</td>\n",
       "      <td>0.109611</td>\n",
       "      <td>-1.465696</td>\n",
       "      <td>-0.033068</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.565646</td>\n",
       "      <td>-0.376636</td>\n",
       "      <td>-0.389352</td>\n",
       "      <td>0.796484</td>\n",
       "      <td>0.069480</td>\n",
       "      <td>0.411968</td>\n",
       "      <td>0.808051</td>\n",
       "      <td>0.978345</td>\n",
       "      <td>0.280803</td>\n",
       "      <td>0.242483</td>\n",
       "      <td>0.710215</td>\n",
       "      <td>1.463319</td>\n",
       "      <td>-0.488809</td>\n",
       "      <td>0.659116</td>\n",
       "      <td>2.040899</td>\n",
       "      <td>0.413254</td>\n",
       "      <td>1.128831</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>1.831824</td>\n",
       "      <td>0.678784</td>\n",
       "      <td>0.461269</td>\n",
       "      <td>0.819580</td>\n",
       "      <td>0.796951</td>\n",
       "      <td>-0.427502</td>\n",
       "      <td>-0.318685</td>\n",
       "      <td>-0.264619</td>\n",
       "      <td>-0.073792</td>\n",
       "      <td>1.567347</td>\n",
       "      <td>0.145724</td>\n",
       "      <td>0.456878</td>\n",
       "      <td>-0.023279</td>\n",
       "      <td>0.530322</td>\n",
       "      <td>1.483768</td>\n",
       "      <td>0.636709</td>\n",
       "      <td>-0.563597</td>\n",
       "      <td>-0.828064</td>\n",
       "      <td>-0.598701</td>\n",
       "      <td>-0.625653</td>\n",
       "      <td>-0.810607</td>\n",
       "      <td>-0.975171</td>\n",
       "      <td>-0.226147</td>\n",
       "      <td>-0.275249</td>\n",
       "      <td>-0.060692</td>\n",
       "      <td>0.117830</td>\n",
       "      <td>0.084099</td>\n",
       "      <td>0.020054</td>\n",
       "      <td>0.030628</td>\n",
       "      <td>0.043376</td>\n",
       "      <td>0.028681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32496017</th>\n",
       "      <td>0.726119</td>\n",
       "      <td>-0.738978</td>\n",
       "      <td>-0.543837</td>\n",
       "      <td>-0.127495</td>\n",
       "      <td>0.738317</td>\n",
       "      <td>-0.016428</td>\n",
       "      <td>0.349479</td>\n",
       "      <td>0.261747</td>\n",
       "      <td>1.239655</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>522</td>\n",
       "      <td>-0.284493</td>\n",
       "      <td>-0.111495</td>\n",
       "      <td>-0.307370</td>\n",
       "      <td>-0.666224</td>\n",
       "      <td>-0.526006</td>\n",
       "      <td>-0.703370</td>\n",
       "      <td>1.702379</td>\n",
       "      <td>-0.828881</td>\n",
       "      <td>-0.261229</td>\n",
       "      <td>-0.142134</td>\n",
       "      <td>1.071015</td>\n",
       "      <td>0.095017</td>\n",
       "      <td>0.441871</td>\n",
       "      <td>-0.060235</td>\n",
       "      <td>-0.406465</td>\n",
       "      <td>1.331972</td>\n",
       "      <td>1.533296</td>\n",
       "      <td>-0.759594</td>\n",
       "      <td>-0.765753</td>\n",
       "      <td>-0.111876</td>\n",
       "      <td>0.225191</td>\n",
       "      <td>-0.501364</td>\n",
       "      <td>0.095531</td>\n",
       "      <td>0.541944</td>\n",
       "      <td>-0.419761</td>\n",
       "      <td>1.187102</td>\n",
       "      <td>0.986660</td>\n",
       "      <td>-0.395733</td>\n",
       "      <td>0.387021</td>\n",
       "      <td>0.534711</td>\n",
       "      <td>0.483614</td>\n",
       "      <td>-0.260048</td>\n",
       "      <td>0.127165</td>\n",
       "      <td>-0.377836</td>\n",
       "      <td>-0.915420</td>\n",
       "      <td>-0.005872</td>\n",
       "      <td>-0.049700</td>\n",
       "      <td>0.013723</td>\n",
       "      <td>-0.492871</td>\n",
       "      <td>-0.406338</td>\n",
       "      <td>-0.043089</td>\n",
       "      <td>0.749103</td>\n",
       "      <td>-0.521930</td>\n",
       "      <td>0.799627</td>\n",
       "      <td>-0.034946</td>\n",
       "      <td>-1.414886</td>\n",
       "      <td>0.073785</td>\n",
       "      <td>-0.142013</td>\n",
       "      <td>0.054707</td>\n",
       "      <td>0.796951</td>\n",
       "      <td>-0.452743</td>\n",
       "      <td>-0.292108</td>\n",
       "      <td>-0.404927</td>\n",
       "      <td>1.215832</td>\n",
       "      <td>-0.188363</td>\n",
       "      <td>-0.254359</td>\n",
       "      <td>-0.146385</td>\n",
       "      <td>-0.454595</td>\n",
       "      <td>-0.273597</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>-0.223208</td>\n",
       "      <td>-0.366105</td>\n",
       "      <td>-0.271781</td>\n",
       "      <td>-0.175845</td>\n",
       "      <td>-0.219294</td>\n",
       "      <td>-0.326256</td>\n",
       "      <td>-0.485207</td>\n",
       "      <td>0.461096</td>\n",
       "      <td>0.155033</td>\n",
       "      <td>-0.010951</td>\n",
       "      <td>-0.541191</td>\n",
       "      <td>-0.296320</td>\n",
       "      <td>-0.021512</td>\n",
       "      <td>0.045422</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.136612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature_00  feature_01  feature_02  feature_03  feature_04  \\\n",
       "32496013   -0.273723   -0.628400    0.021183    0.326928    0.120378   \n",
       "32496014    0.516346   -0.358216    0.170515    0.457404    0.704279   \n",
       "32496015   -0.276064   -0.562562    0.126326    0.341234    0.648612   \n",
       "32496016    0.048198   -0.705530   -0.000436    0.377266   -0.100474   \n",
       "32496017    0.726119   -0.738978   -0.543837   -0.127495    0.738317   \n",
       "\n",
       "          feature_05  feature_06  feature_07  feature_08  feature_09  \\\n",
       "32496013   -0.017101    0.293846    0.267356    0.739021          42   \n",
       "32496014   -0.016830    0.200757    0.196041    1.106106          11   \n",
       "32496015   -0.010778    0.217542    0.150108    0.763144          68   \n",
       "32496016   -0.014540    0.407335    0.410498    0.778039          34   \n",
       "32496017   -0.016428    0.349479    0.261747    1.239655          50   \n",
       "\n",
       "          feature_10  feature_11  feature_12  feature_13  feature_14  \\\n",
       "32496013           5         150    0.886931    0.800840    0.619701   \n",
       "32496014           7          76    0.569555    0.320779    0.445927   \n",
       "32496015           6         388   -0.154750    0.241596   -0.084922   \n",
       "32496016           4         214    0.412368    1.015218    0.308765   \n",
       "32496017           1         522   -0.284493   -0.111495   -0.307370   \n",
       "\n",
       "          feature_15  feature_16  feature_17  feature_18  feature_19  \\\n",
       "32496013   -0.944317   -0.731467   -0.826058    1.270973    2.309707   \n",
       "32496014    0.833764    0.766083    0.760303    2.978000    0.180232   \n",
       "32496015   -0.210215   -0.473479   -0.281191    0.932661   -1.085921   \n",
       "32496016   -0.950215   -0.248845   -0.995436    0.486453    1.312123   \n",
       "32496017   -0.666224   -0.526006   -0.703370    1.702379   -0.828881   \n",
       "\n",
       "          feature_20  feature_21  feature_22  feature_23  feature_24  \\\n",
       "32496013   -0.168244   -0.180836   -0.228961   -0.731446    0.499912   \n",
       "32496014   -0.071296    0.833885   -0.442648    1.909253    1.260226   \n",
       "32496015   -0.415765    0.663795   -0.465737    1.354075    2.110133   \n",
       "32496016   -1.498882   -0.192676   -0.928796   -1.160311    0.198818   \n",
       "32496017   -0.261229   -0.142134    1.071015    0.095017    0.441871   \n",
       "\n",
       "          feature_25  feature_26  feature_27  feature_28  feature_29  \\\n",
       "32496013   -0.058762   -1.233727    0.421452    1.658583   -0.643178   \n",
       "32496014    2.173384    0.124759   -1.742508   -2.060564    2.431904   \n",
       "32496015    2.677571   -0.370774   -2.367140   -1.442387    0.450709   \n",
       "32496016   -0.378694    0.325012   -1.749679   -1.901364   -0.891058   \n",
       "32496017   -0.060235   -0.406465    1.331972    1.533296   -0.759594   \n",
       "\n",
       "          feature_30  feature_31  feature_32  feature_33  feature_34  \\\n",
       "32496013   -0.641713   -0.127338    1.012934   -0.808432    0.640629   \n",
       "32496014    0.894903    1.336326    0.233143    0.268164    0.180035   \n",
       "32496015   -0.131176    0.483004    0.787535   -0.551896    0.947080   \n",
       "32496016   -0.919408   -0.148865   -0.459053   -0.338284    0.518999   \n",
       "32496017   -0.765753   -0.111876    0.225191   -0.501364    0.095531   \n",
       "\n",
       "          feature_35  feature_36  feature_37  feature_38  feature_39  \\\n",
       "32496013    0.046857   -0.553982   -0.097041   -0.199727   -0.778504   \n",
       "32496014   -0.047150   -0.035961    1.073607    0.868699    1.179430   \n",
       "32496015    0.807590   -0.549592    0.100099    0.098323    0.180247   \n",
       "32496016    0.109611   -1.465696   -0.033068    0.006976    0.565646   \n",
       "32496017    0.541944   -0.419761    1.187102    0.986660   -0.395733   \n",
       "\n",
       "          feature_40  feature_41  feature_42  feature_43  feature_44  \\\n",
       "32496013    0.546700    0.259686   -0.897121   -0.245391   -0.181529   \n",
       "32496014    0.910744    0.365738   -0.806121   -1.221758   -0.294194   \n",
       "32496015    1.340250   -0.481007    1.146693   -0.012450    0.033310   \n",
       "32496016   -0.376636   -0.389352    0.796484    0.069480    0.411968   \n",
       "32496017    0.387021    0.534711    0.483614   -0.260048    0.127165   \n",
       "\n",
       "          feature_45  feature_46  feature_47  feature_48  feature_49  \\\n",
       "32496013   -0.041429   -0.114340    0.074312   -0.162039   -0.156312   \n",
       "32496014   -0.309661   -1.669550   -1.185478   -0.013506   -0.296199   \n",
       "32496015   -0.014503    0.447920    0.086581   -0.195873    0.004835   \n",
       "32496016    0.808051    0.978345    0.280803    0.242483    0.710215   \n",
       "32496017   -0.377836   -0.915420   -0.005872   -0.049700    0.013723   \n",
       "\n",
       "          feature_50  feature_51  feature_52  feature_53  feature_54  \\\n",
       "32496013   -0.445739   -0.194293   -0.992565    0.026359    0.776747   \n",
       "32496014    0.629851    1.619304    0.130224   -1.331440   -1.206886   \n",
       "32496015    0.530240   -0.618310   -0.746920    0.534145   -0.556152   \n",
       "32496016    1.463319   -0.488809    0.659116    2.040899    0.413254   \n",
       "32496017   -0.492871   -0.406338   -0.043089    0.749103   -0.521930   \n",
       "\n",
       "          feature_55  feature_56  feature_57  feature_58  feature_59  \\\n",
       "32496013    0.515383   -0.310368    0.715174    0.382011   -0.057844   \n",
       "32496014   -0.506745   -0.665731   -1.265526   -1.339285    0.179611   \n",
       "32496015    0.391937    0.606587    1.020363    0.241115   -0.063700   \n",
       "32496016    1.128831    0.767357    1.831824    0.678784    0.461269   \n",
       "32496017    0.799627   -0.034946   -1.414886    0.073785   -0.142013   \n",
       "\n",
       "          feature_60  feature_61  feature_62  feature_63  feature_64  \\\n",
       "32496013   -0.040984    0.796951   -0.526368   -0.235048   -0.354705   \n",
       "32496014   -0.155559    0.796951   -0.099937    0.127875   -0.027559   \n",
       "32496015    0.080730    0.796951   -0.368261   -0.287799   -0.280969   \n",
       "32496016    0.819580    0.796951   -0.427502   -0.318685   -0.264619   \n",
       "32496017    0.054707    0.796951   -0.452743   -0.292108   -0.404927   \n",
       "\n",
       "          feature_65  feature_66  feature_67  feature_68  feature_69  \\\n",
       "32496013    1.505933    1.735560    0.519725    0.034015    0.247436   \n",
       "32496014    2.582265   -0.386582    1.081167   -0.309833    0.429599   \n",
       "32496015    0.449343    1.240352   -0.351033    0.879358   -0.191713   \n",
       "32496016   -0.073792    1.567347    0.145724    0.456878   -0.023279   \n",
       "32496017    1.215832   -0.188363   -0.254359   -0.146385   -0.454595   \n",
       "\n",
       "          feature_70  feature_71  feature_72  feature_73  feature_74  \\\n",
       "32496013    1.266712    1.023329    1.049752   -0.020475    0.000255   \n",
       "32496014    0.702698    0.696585    0.391108   -0.312883   -0.507126   \n",
       "32496015   -0.081830   -0.143664    0.025583   -0.265242   -0.402117   \n",
       "32496016    0.530322    1.483768    0.636709   -0.563597   -0.828064   \n",
       "32496017   -0.273597    0.011529   -0.223208   -0.366105   -0.271781   \n",
       "\n",
       "          feature_75  feature_76  feature_77  feature_78  responder_0_lag_1  \\\n",
       "32496013   -0.234894   -0.328479   -0.167722   -0.107061           0.013119   \n",
       "32496014   -0.303390   -0.442164   -0.255057   -0.366450          -0.150453   \n",
       "32496015   -0.290549   -0.336317   -0.207405   -0.323209          -0.063781   \n",
       "32496016   -0.598701   -0.625653   -0.810607   -0.975171          -0.226147   \n",
       "32496017   -0.175845   -0.219294   -0.326256   -0.485207           0.461096   \n",
       "\n",
       "          responder_1_lag_1  responder_2_lag_1  responder_3_lag_1  \\\n",
       "32496013           0.299852           0.113701           0.191003   \n",
       "32496014           0.697600          -2.491776           0.146255   \n",
       "32496015           0.286257           1.291592           0.117581   \n",
       "32496016          -0.275249          -0.060692           0.117830   \n",
       "32496017           0.155033          -0.010951          -0.541191   \n",
       "\n",
       "          responder_4_lag_1  responder_5_lag_1  responder_6_lag_1  \\\n",
       "32496013           0.090730          -0.233323          -0.171773   \n",
       "32496014           0.055375          -0.119033           0.025655   \n",
       "32496015           0.050176          -0.013990          -0.005065   \n",
       "32496016           0.084099           0.020054           0.030628   \n",
       "32496017          -0.296320          -0.021512           0.045422   \n",
       "\n",
       "          responder_7_lag_1  responder_8_lag_1  \n",
       "32496013          -0.042368          -0.313162  \n",
       "32496014           0.050160           0.044680  \n",
       "32496015           0.032325          -0.019257  \n",
       "32496016           0.043376           0.028681  \n",
       "32496017           0.061172           0.136612  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttraining's l2: 0.752714\tvalid_1's l2: 0.662428\n",
      "[20]\ttraining's l2: 0.750693\tvalid_1's l2: 0.661217\n",
      "[30]\ttraining's l2: 0.749317\tvalid_1's l2: 0.660341\n",
      "[40]\ttraining's l2: 0.748268\tvalid_1's l2: 0.659631\n",
      "[50]\ttraining's l2: 0.74732\tvalid_1's l2: 0.659018\n",
      "[60]\ttraining's l2: 0.746581\tvalid_1's l2: 0.658598\n",
      "[70]\ttraining's l2: 0.745827\tvalid_1's l2: 0.658284\n",
      "[80]\ttraining's l2: 0.74517\tvalid_1's l2: 0.658075\n",
      "[90]\ttraining's l2: 0.744618\tvalid_1's l2: 0.657888\n",
      "[100]\ttraining's l2: 0.744009\tvalid_1's l2: 0.657727\n",
      "[110]\ttraining's l2: 0.743517\tvalid_1's l2: 0.657531\n",
      "[120]\ttraining's l2: 0.742888\tvalid_1's l2: 0.657485\n",
      "[130]\ttraining's l2: 0.742405\tvalid_1's l2: 0.657417\n",
      "[140]\ttraining's l2: 0.741977\tvalid_1's l2: 0.657348\n",
      "[150]\ttraining's l2: 0.741394\tvalid_1's l2: 0.657393\n",
      "[160]\ttraining's l2: 0.740866\tvalid_1's l2: 0.657401\n",
      "[170]\ttraining's l2: 0.740513\tvalid_1's l2: 0.65735\n",
      "Best iteration: 142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Weighted R2 score is: 0.010574732592161684\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb_online_learning(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d5b42e-8908-4589-9756-d4fb977ef6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Kaggle\\kaggle_venvs\\ml\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The LGBMRegressor or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(device=&#x27;gpu&#x27;, early_stopping_round=30, feature_fraction=0.8,\n",
       "              lambda_l2=100, learning_rate=0.05, n_estimators=90000,\n",
       "              verbosity=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMRegressor</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(device=&#x27;gpu&#x27;, early_stopping_round=30, feature_fraction=0.8,\n",
       "              lambda_l2=100, learning_rate=0.05, n_estimators=90000,\n",
       "              verbosity=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(device='gpu', early_stopping_round=30, feature_fraction=0.8,\n",
       "              lambda_l2=100, learning_rate=0.05, n_estimators=90000,\n",
       "              verbosity=-1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8d3dc-c225-4c0d-a41f-3da3b887045f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efbf23-81e0-4e6f-a346-3e01ee558f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746715e-6673-4ba1-953c-41d00f590a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb20dd-2150-426f-8739-2f63402dc7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4651e-f019-4e55-8ce4-b03aae6c1c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c8234-33e1-4891-9527-58d15c529e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47aa04-b2ad-48db-a1b4-1939520ab885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc310b8-2089-42ee-8c3b-80545060deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c3859c-b79e-426f-9799-fffdb93d3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "with open(models_path + \"lgb_model.pkl\", 'wb') as file:\n",
    "    pickle.dump(lgb_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6684c-98f8-447c-8b58-28f1d21aef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "with open(f\"{models_path}/lgb_model.pkl\", \"rb\") as f:\n",
    "    lgb_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df7914-ec21-4603-9388-57043c98ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = train_df.filter(pl.col('date_id') > 1300)\n",
    "print(val_df.shape)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1e705-b82e-45f8-b992-bdb9595408a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389887f7-ef29-40d3-9595-f4af0b63b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_online_learning(val_data, current_model, optuna_n_trials):\n",
    "    val_data = val_data.clone()\n",
    "    print('this is the initial validation data')\n",
    "    print(val_data.shape)\n",
    "    display(val_data.head())\n",
    "    display(val_data.tail())\n",
    "    i = 0\n",
    "    val_date_ids = sorted(val_data['date_id'].unique())\n",
    "    for date_id_v in val_date_ids:\n",
    "        date_id_df = val_data.filter(pl.col('date_id') == date_id_v)\n",
    "        print('this is date_id_df')\n",
    "        print(date_id_df.shape)\n",
    "        display(date_id_df.head())\n",
    "        display(date_id_df.tail())\n",
    "        \n",
    "        X_train = date_id_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "        y_train = date_id_df['responder_6'].to_pandas()\n",
    "        weights_train = date_id_df['weight'].to_pandas()\n",
    "\n",
    "        #train_dataset = lgb.Dataset(data=X_train, label=y_train, weight=weights_train)\n",
    "\n",
    "        val_data = val_data[date_id_df.shape[0]:]\n",
    "        print('this is updated validation data')\n",
    "        print(val_data.shape)\n",
    "        display(val_data.head())\n",
    "        display(val_data.tail())\n",
    "\n",
    "        X_val = val_data.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n",
    "        y_val = val_data['responder_6'].to_pandas()\n",
    "        weights_val = val_data['weight'].to_pandas()\n",
    "\n",
    "        #val_dataset = lgb.Dataset(data=X_val, label=y_val, weight=weights_val)\n",
    "\n",
    "        '''base_params = {\n",
    "            'verbosity': -1,\n",
    "            'learning_rate': 1,\n",
    "            #'feature_fraction': 0.8,\n",
    "            'device': 'gpu',\n",
    "            'early_stopping_round': 30,\n",
    "            #'lambda_l2': 100\n",
    "        }'''\n",
    "\n",
    "        '''updated_model = lgb.train(\n",
    "            params=base_params,\n",
    "            train_set=train_dataset,\n",
    "            valid_sets=[train_dataset, val_dataset],\n",
    "            num_boost_round=90,\n",
    "            init_model=current_model,\n",
    "            callbacks=[log_evaluation(period=50), record_evaluation()]\n",
    "        )'''\n",
    "    \n",
    "        '''online_model = LGBMRegressor(\n",
    "            **base_params,\n",
    "            n_estimators=100000\n",
    "        )'''\n",
    "\n",
    "        '''current_model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val], callbacks=[log_evaluation(period=10)], init_model=current_model)\n",
    "        #current_model.fit(X_val, y_val, sample_weight=weights_val, eval_set=[(X_val, y_val), (X_train, y_train)], eval_sample_weight=[weights_val, weights_train], callbacks=[log_evaluation(period=10)], init_model=current_model)\n",
    "\n",
    "        #display(online_model)\n",
    "\n",
    "        plt.figure()\n",
    "        lgb.plot_metric(current_model)\n",
    "        plt.ylim(0, 2)\n",
    "        plt.show()\n",
    "        \n",
    "        val_preds = current_model.predict(X_val)\n",
    "        \n",
    "        return current_model'''\n",
    "\n",
    "        base_params = {\n",
    "            'verbosity': -1,\n",
    "            #'learning_rate': 0.05,\n",
    "            #'feature_fraction': 0.8,\n",
    "            'device': 'gpu',\n",
    "            'early_stopping_round': 10,\n",
    "            #'lambda_l2': 100,\n",
    "            'seed': 42\n",
    "        }\n",
    "\n",
    "        def objective(trial):\n",
    "\n",
    "            params_to_tune = {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.03),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 300),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 4000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 3),\n",
    "                'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 0.3),\n",
    "                'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),\n",
    "                'lambda_l2': trial.suggest_float('lambda_l2', 1000, 2000)\n",
    "            }\n",
    "\n",
    "            online_model = LGBMRegressor(\n",
    "                **base_params,\n",
    "                **params_to_tune,\n",
    "                n_estimators=100000\n",
    "            )\n",
    "\n",
    "            online_model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val], init_model=current_model)\n",
    "            #online_model.fit(X_val, y_val, sample_weight=weights_val, eval_set=[(X_val, y_val), (X_train, y_train)], eval_sample_weight=[weights_val, weights_train], callbacks=[log_evaluation(period=10)], init_model=current_model)\n",
    "\n",
    "            '''plt.figure()\n",
    "            lgb.plot_metric(online_model)\n",
    "            plt.ylim(0, 2)\n",
    "            plt.show()'''\n",
    "\n",
    "            #best_iteration = online_model.best_iteration_\n",
    "            #print(f\"Best iteration: {best_iteration}\")\n",
    "\n",
    "            val_preds = online_model.predict(X_val)\n",
    "\n",
    "            val_r2_score = r2_score(y_val, val_preds, sample_weight=weights_val)\n",
    "\n",
    "            return val_r2_score\n",
    "\n",
    "        with tqdm(total=optuna_n_trials, desc=\"Optimizing\", unit=\"trial\") as pbar:\n",
    "    \n",
    "            # Define a callback function to update the progress bar\n",
    "            def progress_bar_callback(study, trial):\n",
    "                pbar.update(1)\n",
    "        \n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials=optuna_n_trials, callbacks=[progress_bar_callback])\n",
    "\n",
    "        return study\n",
    "    \n",
    "        best_params = study.best_params\n",
    "\n",
    "        online_model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val], callbacks=[log_evaluation(period=10)], init_model=current_model)\n",
    "\n",
    "        display(online_model)\n",
    "\n",
    "        plt.figure()\n",
    "        lgb.plot_metric(online_model)\n",
    "        plt.ylim(0, 2)\n",
    "        plt.show()\n",
    "        \n",
    "        val_preds = online_model.predict(X_val)\n",
    "\n",
    "        print('Val Weighted R2 score is:', r2_score(y_val, val_preds, sample_weight=weights_val))\n",
    "\n",
    "        return online_model\n",
    "\n",
    "        if i > 20:\n",
    "            return\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82f864-8066-48e1-9afb-184bc933850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_study = val_online_learning(val_df, lgb_model, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd780c53-9568-480d-bc02-b28fc79566d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in lgb_study.best_params.keys():\n",
    "    fig = plot_slice(lgb_study, params=[param])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae940b42-2237-4577-bbe4-5a3b08bec314",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(lgb_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157c449-2f9c-45ae-a31a-c7a2e4c7c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5b2e5-0bce-41c2-8fcc-dd6e22a00aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa6a0f-3cc5-45a6-a111-d7f054c262be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df = pd.DataFrame({k:[lgb_study.best_params[k]] for k in lgb_study.best_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e7532-c40e-4ea6-8e4d-f4a0d96650d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69b03b-80e1-45c0-b742-063d9e9ee361",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df.to_csv(models_path + 'lgb_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b4c48-301e-4940-934e-b471272dd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_dict(params_df):\n",
    "    params_dict = {}\n",
    "    for col in params_df.columns:\n",
    "        v = params_df[col][0]\n",
    "        if type(v) == np.int64:\n",
    "            v = int(v)\n",
    "        params_dict[col] = v\n",
    "\n",
    "    return params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e61b03-9287-4a98-bee1-0259004823b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_dict = create_params_dict(lgb_params_df)\n",
    "lgb_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd110dc-19db-4002-9675-c5206b3ba329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b6282-1d42-4d92-a501-529f7da27e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119ec0b-151f-4c3e-a44f-909c59d8b25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66242a14-9e00-4e9c-96a9-f7b2c09d3ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e0e9d-517b-4ee8-8998-438e28026201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f98a4-a2dc-475d-ac3e-14c62606e802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321b527-d9a1-4977-94bf-661d62e83be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5bfc2-0c8d-4dc5-9c1f-a02595836d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_df.drop(['date_id', 'time_id', 'symbol_id']).columns\n",
    "imp_df = pd.DataFrame(sorted(zip(cols, first_shap_importance)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459cef0-80a6-4b90-9918-332b449c380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506da0e-1120-4d22-99b8-31da7adc1a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7e244-dc03-4612-a335-bd7366f6a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(imp_df['Feature'], imp_df['Importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c5ce2-6075-47e7-8bf5-98468f451b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "unimportant_df = imp_df[imp_df['Importance'] <= imp_df['Importance'].quantile(0.3)]\n",
    "unimportant_cols = unimportant_df['Feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc98e4-b6c4-41a2-b1bf-e63e16828ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected_df = train_df.drop(unimportant_cols)\n",
    "print(train_selected_df.shape)\n",
    "train_selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8c121-661f-4c90-8e3b-c9aa62bf2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_shap_importance = lgb_train(train_selected_df, y_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c749891-7ec2-4ea7-b4a3-40691e436f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
